import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import signal_processing
from typing import Literal
import time
from joblib import load
from sklearn.metrics import classification_report
import autosklearn.metrics as metrics

def shapley_value(x_train:np.ndarray, x_test:np.ndarray, y_train:np.ndarray, y_test:np.ndarray, X:pd.DataFrame):
    from sklearn.metrics import accuracy_score, mean_squared_error
    import shap
    from xgboost.sklearn import XGBRegressor

    shap.initjs()
    xgb_model = XGBRegressor(n_estimators=1000, max_depth=10, learning_rate=0.001, random_state=0)
    xgb_model.fit(x_train, y_train)
    y_predict = xgb_model.predict(x_test)
    print(y_predict)
    mean_squared_error(y_test, y_predict)**(0.5)
    explainer = shap.TreeExplainer(xgb_model)
    shap_values = explainer.shap_values(x_train)
    shap.summary_plot(shap_values, features=x_train, feature_names=X.columns)
    acc = accuracy_score(y_test, y_predict)
    print(acc)

def fisher_score_show(x_train:np.ndarray, x_test:np.ndarray, y_train:np.ndarray, y_test:np.ndarray):
    from skfeature.function.similarity_based import fisher_score
    from sklearn import svm
    from sklearn.metrics import accuracy_score

    score = fisher_score.fisher_score(x_train, y_train)
    idx = fisher_score.feature_ranking(score)
    num_fea = 5
    
    plt.figure(layout="constrained")
    plt.bar(range(len(score)), score)
    plt.xlabel('Feature Index')
    plt.ylabel('Fisher Score')
    plt.title('Fisher Score for Each Feature')
    for i in range(num_fea):
        plt.annotate(idx[i], 
                     xy=(idx[i], score[idx[i]]), rotation=0, xycoords='data',
                     xytext=(0,0), textcoords='offset pixels')
    plt.show()
    
    selected_feature_train = x_train[:, idx[0:num_fea]]
    selected_feature_test = x_test[:, idx[0:num_fea]]
    clf = svm.LinearSVC(dual=False)
    clf.fit(selected_feature_train, y_train)
    y_predict = clf.predict(selected_feature_test)
    acc = accuracy_score(y_test, y_predict)
    print(acc)

def compare_target_predict(df:pd.DataFrame, target:str, predict:str):
    '''
    show wrong prediction spectrum

    Parameters
    ----------
    df : input dataframe
    target : str
        name pf target column
    predict : str
        name of predict column
    
    Examples
    --------
    >>> compare_target_predict(df, 'Type', 'Predict')
    '''
    df_wrong = df.loc[df[target] != df[predict], :]
    fig, axs = plt.subplots(1,1, layout='constrained')
    for i in df_wrong.index:
        axs.plot(df_wrong.loc[i][:151], label= i + ': target %i predict %i'%(df_wrong.loc[i, 'Type'], df_wrong.loc[i, 'Predict']))
    axs.set_xticks(np.arange(0,151, step=10))
    axs.set_yscale('log')
    axs.legend()
    plt.show()

# (Moved above to fix default argument error)
def label_transfer(sample_num: str):
    '''
    transfer sample number to label 0 and 1
    '''
    return 0 if signal_processing.class_label(sample_num) == 0 else 1

def predict(psd_file:str, joblib:str, keyword:str, col:int, stats:bool, label_method = label_transfer, df=None):
    '''
    load psd spectrum, and load model, output the predict result

    Args:
        df(pd.DataFrame):
            dataframe containing the psd spectrum data. The columns are the order number of the psd spectrum,
            the sensor channel, and the sample number. The index is shuffled.
            If None, the data will be loaded from `psd_file`.
        psd_file(str):
            psd spectrum generated by `acc_processing_excel()`. The samples are saved in separated sheets, 
            the column label in each sheets is the sensor channel, and the index is the order number.
            The psd result is averaged by windows.
        joblib(str): 
            file name of the trained classification model
        keyword(str):
            selected sensor channel
        col(int):
            controls the feature number to match the model feature number
        stats(bool):
            whether to add additional features such as mean and std of specific features.
            If True, the feature number will be increased by 2.
    Examples:
        >>> predict(psd_file='../../test_data//20250410_test_samples//psd_20%//psd_high_resolution.xlsx',
                    joblib='../../model//20duty_high_resolution//lr_left_high_resolution_set1_5121_v1.joblib',
                    keyword='lr_left', col=5121, stats=False)
    '''
    # 匯入模型
    clf = load(joblib)
    if df is None:
        df = load_data(window=False, format='excel', dir=psd_file, keyword=keyword)

    res = df['sample_num'].to_frame()
    res['label'] = df['sample_num'].apply(label_method)

    # 模型預測測試
    t1 = time.time()
    df = preprocess_features(df, col=col, stats=stats)
    res['predict'] = clf.predict(df.drop(columns=['sample_num']))
    t2 = time.time()
    total_inference_time_ms = (t2 - t1) * 1000
    
    print(joblib.split('/')[-1][:-7])
    print(res)
    print(classification_report(res['label'], res['predict']))
    print(f'Total inference time: {total_inference_time_ms:.2f} ms')
        
def predict_single(psd_file:str, joblib:str, keyword:str, col:int, stats:bool, label_method = label_transfer):
    # 匯入模型
    clf = load(joblib)
    
    df = load_data(window=False, format='excel', dir=psd_file, keyword=keyword)

    total_samples = df['sample_num'].shape[0]
    total_inference_time_ms = 0
    res = df['sample_num'].to_frame()
    res['label'] = df['sample_num'].apply(label_method)

    for i in range(total_samples):
        single_sample = df.iloc[i, :].to_frame().transpose()
        # 模型預測測試
        t1 = time.time()
        single_sample = preprocess_features(single_sample, col=col, stats=stats)
        target = clf.predict(single_sample.drop(columns=['sample_num']))
        t2 = time.time()
        total_inference_time_ms += (t2 - t1) * 1000
        res.loc[i, 'predict'] = target
    
    print(joblib.split('/')[-1][:-7])
    print(res)
    print(classification_report(res['label'], res['predict']))
    print(f'Total inference time: {total_inference_time_ms:.2f} ms')
    print(f'Average inference time per sample: {total_inference_time_ms / total_samples:.2f} ms')
    
def load_data(format:Literal['excel', 'parquet'], dir:str, keyword:str, window:bool = True):
    '''
    load data of chosen format and filtered with keyword.
    Args:
        format (Literal['excel', 'parquet']):
            the format of the data, either 'excel' or 'parquet', which contains the psd spectrum.
        dir (str): the directory of the parquet file or the filename of the excel file.
        keyword (str): the channel of sensor, such as 'lr_left', 'lr_right', etc.
        window (bool): whether the data is windowed, default is True. If True, the data is read from un-averaged psd spectrum.
        
    Returns:
        pd.DataFrame: a dataframe containing the psd spectrum data. The columns are the order number of the psd spectrum,
        the sensor channel, and the sample number. The index is shuffled.
    Examples:
        >>> df = load_data(format='parquet', dir='../../test_data//psd_20%//psd_window_high_resolution_20%//', keyword='lr_left')
        >>> df.head()
    Raises:
        ValueError: if the dataframe contains NaN values.
    '''
    if format == 'excel':
        df = pd.DataFrame()
        if not window:
            df = signal_processing.read_sheets(filename=dir, usecols=[0,1,2,3], combine=True)
            df = df.transpose()
            df['name'] = df.index
            df = df.reset_index(drop=True)
        
        else:
            df = signal_processing.read_sheets(filename=dir, combine=True, axis=0)
        
        # add columns to describe the sensor channel and the sample_num
        df['channel'] = [name[7:] for name in df['name']]
        df['sample_num'] = [name[:6] for name in df['name']]
        # select a particular channel and shuffle
        df = df.loc[df['channel'] == keyword].drop(columns='channel').sample(frac = 1).reset_index(drop=True)
    
    else:
        # select a particular channel and shuffle
        df = signal_processing.read_parquet_keyword(keyword, dir, 
                                                parse_func=signal_processing.parse_digital).sample(frac=1).reset_index(drop=True)
    if df.isna().any().any():
        raise ValueError('nan values exist in the teat data.')
    return df
    
    
def preprocess_features(df:pd.DataFrame, col:int, stats:bool=False):
    '''
    preprocessing

    Parameters
    ----------
    df : dataframe contains 'sample_num' 
    col : preserved column number
    stats : bool
        whether to add additional features such as mean and std of specific features
    '''
    # add additional features such as mean and std of specific features
    if stats:
        df = signal_processing.calculate_spectral_stats(30, 80, df)
        # drop unused columns to reduce feature number
        df = pd.concat([df.iloc[:, :col], df.iloc[:, -3:]], axis=1)
    else:
        df = pd.concat([df.iloc[:, :col], df['sample_num']], axis=1)
    # transfer the column label into string before training
    df = signal_processing.cast_column_to_str(df, 2)
    if df.isna().any().any():
        raise ValueError('nan values exist in the teat data.')
        
    return df

def train_test_split(df:pd.DataFrame, test_samples: list):
    '''
    separate train and test set

    Parameters
    ----------
    df : dataframe contains 'sample_num'
    '''
    X_train = df.loc[[x not in test_samples for x in df['sample_num']]]
    X_test = df.loc[[x in test_samples for x in df['sample_num']]]

    y_train = np.array([label_transfer(sample_num) for sample_num in X_train['sample_num']])
    y_test = np.array([label_transfer(sample_num) for sample_num in X_test['sample_num']])

    # encode categorical columns
    y_train = pd.DataFrame(y_train, dtype="category")
    y_test = pd.DataFrame(y_test, dtype="category")

    # drop target
    X_train = X_train.drop(columns='sample_num').reset_index(drop=True)
    X_test = X_test.drop(columns='sample_num').reset_index(drop=True)
    print('train shape:', X_train.shape)
    print('test shape:', X_test.shape)
    return X_train, X_test, y_train, y_test

def model_info(model_file_name: str, leaderboard:bool = True, show_models:bool = True, sprint_statistics:bool = True):
    '''
    load a trained model and print model info
    Args:
        model_file_name (str): 
            the file name of the trained model, which is a joblib file.
        leaderboard (bool):
            whether to print the leaderboard of the model.
        show_models (bool): 
            whether to show the models in detail.
        sprint_stats (bool): 
            whether to print the sprint statistics of the model.
    '''
    model = load(model_file_name)
    
    if leaderboard:
        df = model.leaderboard(detailed = True, ensemble_only=True)
        print(df)
    # detail of the model
    if show_models:
        best_model_info = model.show_models()
        print(best_model_info)
    if sprint_statistics:
        print(model.sprint_statistics())
    
def model_training_log(model, model_file_name: str, X_train:pd.DataFrame, y_train:pd.DataFrame, X_test:pd.DataFrame, y_test:pd.DataFrame,
                       actual_training_time:float, dataset_name:str, train_test_split:int, channel:str):
    '''
    print model training log for convenience pasting on table
    reference: training data summery table
    Args:
        model: 
            the trained model, which is returned by fit().
        model_file_name (str): 
            the file name of the trained model, which is a joblib file.
        X_train (pd.DataFrame): 
            the training data features.
        y_train (pd.DataFrame): 
            the training data labels.
        X_test (pd.DataFrame): 
            the test data features.
        y_test (pd.DataFrame): 
            the test data labels.
        actual_training_time (float): 
            the actual training time of the model in seconds.
        dataset_name (str): 
            the name of the dataset, which will be used in the model training log and fit().
            the format is 'dataset_id,preprocessing_id', such as 'DS_L_01,PP_HR_400'.
        train_test_split (int): 
            the set number of the training data, such as 1, 2, etc.
        channel (str): 
            the sensor channel to be trained, such as 'lr_left', 'ud_axial', etc.
    '''
    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())
    train_report_dict = classification_report(y_train, model.predict(X_train), output_dict=True)
    test_report_dict = classification_report(y_test, model.predict(X_test), output_dict=True)
    dataset_id = dataset_name.split(',')[0] if dataset_name else 'unknown'
    preprocessing_id = dataset_name.split(',')[1] if len(dataset_name.split(',')) > 1 else 'unknown'

    print(
        f"{timestamp} | {model_file_name} | {model.__class__.__name__} | {model} | {actual_training_time:.2f} |"
        f"{dataset_id} | {preprocessing_id} | {train_test_split} |"
        f"{channel}| | {train_report_dict['accuracy']:.4f} |  | {test_report_dict['accuracy']:.4f} | "
        f"{test_report_dict['weighted avg']['precision']:.4f} | {test_report_dict['weighted avg']['recall']:.4f} | "
        f"{test_report_dict['weighted avg']['f1-score']:.4f} "
    )
    

def train_autosklearn_v1_model(dataset_name:str, model_path:str, set_no:int, channel:str, 
                               X_train, X_test, y_train, y_test, 
                               time_limit = 600, per_run_limit = 200, n_jobs = -1, **kwargs):
    '''
    Trains an AutoSklearnClassifier model with specific optimization and ensemble settings.
    
    Args:
        dataset_name (str): 
            the name of the dataset, which will be used in the model training log and fit().
        model_path (str): 
            the path to save the trained model.
        set_no (int):
            the set number of the training data, such as 1, 2, etc.
        channel (str): 
            the sensor channel to be trained, such as 'lr_left', 'ud_axial', etc.
        X_train (pd.DataFrame): 
            the training data features.
        X_test (pd.DataFrame): 
            the test data features.
        y_train (pd.DataFrame): 
            the training data labels.
        y_test (pd.DataFrame): 
            the test data labels.
        time_limit (int): 
            the total time limit for the model training in seconds. Default is 600 seconds.
        per_run_limit (int): 
            the time limit for each model training run in seconds. Default is 200 seconds.
        n_jobs (int): 
            the number of jobs to run in parallel. Default is -1, which means using all available cores.
        **kwargs:
            additional keyword arguments to be passed to the AutoSklearnClassifier.
    Returns:
        None
    Examples:
        >>> train_autosklearn_v1_model(dataset_name='DS_L_01,PP_HR_400', model_path='model_v1.joblib', set_no=1, channel='lr_left',
                                      X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test,
                                      time_limit=600, per_run_limit=200, n_jobs=-1, 
                                      ensemble_kwargs={'ensemble_size': 5}, ensemble_nbest=10, 
                                      metric=metrics.CLASSIFICATION_METRICS['f1_weighted'])
    '''
    import autosklearn.classification
    automlclassifierV1 = autosklearn.classification.AutoSklearnClassifier(
        time_left_for_this_task=time_limit,
        per_run_time_limit=per_run_limit,
        resampling_strategy='cv',
        resampling_strategy_arguments={'folds': 5},
        memory_limit=None,
        n_jobs=n_jobs,
        **kwargs
    )
    t1 = time.time()
    automlclassifierV1.fit(X_train, y_train, dataset_name=dataset_name)
    t2 = time.time()
    # print log
    model_training_log(automlclassifierV1, model_path.split('/')[-1], 
                       X_train, y_train, X_test, y_test, 
                       actual_training_time=t2-t1, dataset_name=dataset_name, train_test_split=set_no,channel=channel)
    save_model(automlclassifierV1, model_path)
    
def train_autosklearn_v2_model(dataset_name:str, model_path:str, set_no:int, channel:str, 
                               X_train, X_test, y_train, y_test, 
                               time_limit = 600, per_run_limit = 200, n_jobs = -1, **kwargs):
    '''
    Trains an AutoSklearn2Classifier model with specific optimization and ensemble settings.
    
    Note:

    Due to a [known bug](https://github.com/automl/auto-sklearn/issues/1654) in auto-sklearn v0.15.0 
    preventing direct optimization with f1_weighted or f1_macro metrics, the model's primary optimization
    goal (AutoML search process) is set to 'accuracy' by default by leaving `metric` argument as default.

    However, the ensemble building phase explicitly incorporates 'f1_weighted'
    via `ensemble_kwargs`. This ensures that while the overall model selection
    is accuracy-driven, the final ensemble's sub-model weighting prioritizes
    f1_weighted performance, providing a degree of f1-score orientation.
    Args:
        dataset_name (str): 
            the name of the dataset, which will be used in the model training log and fit().
        model_path (str): 
            the path to save the trained model.
        set_no (int):
            the set number of the training data, such as 1, 2, etc.
        channel (str): 
            the sensor channel to be trained, such as 'lr_left', 'ud_axial', etc.
        X_train (pd.DataFrame): 
            the training data features.
        X_test (pd.DataFrame): 
            the test data features.
        y_train (pd.DataFrame): 
            the training data labels.
        y_test (pd.DataFrame): 
            the test data labels.
        time_limit (int): 
            the total time limit for the model training in seconds. Default is 600 seconds.
        per_run_limit (int): 
            the time limit for each model training run in seconds. Default is 200 seconds.
        n_jobs (int): 
            the number of jobs to run in parallel. Default is -1, which means using all available cores.
        **kwargs:
            additional keyword arguments to be passed to the AutoSklearn2Classifier.
    Returns:
        None
    Examples:
        >>> train_autosklearn_v2_model(dataset_name='DS_L_01,PP_HR_400', model_path='model_v2.joblib', set_no=1, channel='lr_left',
                                      X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test,
                                      time_limit=600, per_run_limit=200, n_jobs=-1, 
                                      ensemble_kwargs={'ensemble_size': 5, 'metrics': metrics.CLASSIFICATION_METRICS['f1_weighted']},
                                      ensemble_nbest=10)
    '''
    from autosklearn.experimental.askl2 import AutoSklearn2Classifier

    automlclassifierV2 = AutoSklearn2Classifier(
        time_left_for_this_task=time_limit,
        per_run_time_limit=per_run_limit,
        memory_limit=None,
        n_jobs=n_jobs,
        **kwargs
    )
    t1 = time.time()
    automlclassifierV2.fit(X_train, y_train, dataset_name = dataset_name)
    t2 = time.time()
    # print log
    model_training_log(automlclassifierV2, model_path.split('/')[-1], 
                       X_train, y_train, X_test, y_test, 
                       actual_training_time=t2-t1, dataset_name=dataset_name, train_test_split=set_no, channel=channel)
    save_model(automlclassifierV2, model_path)
    
def save_model(trained_model, path:str):
    from joblib import dump
    dump(trained_model, path)
    print('model saved at:', path)

test_sample = {
    'set1' : ['000027', '000048', '000053', '003735', '004073', '000785'],
    'set2' : ['000030', '000050', '003735', '003861', '001833', '002577'],
    'set3' : ['000030', '000039', '000052', '004072', '004073', '004802']
}

def train_models(dataset_name:str, dir:str, format:Literal['excel', 'parquet'],
                 channel:str, set_no:int, col:int, stats:bool=False, model_save_path:str='', high_resolution:bool=False):
    '''
    train models with autosklearn v1 and v2

    Args:
        dataset_name (str): 
            the name of the dataset, which will be used in the model training log and fit().
        dir (str): 
            directory of the psd spectrum data, which is in parquet format.
        channel (str):
            the sensor channel to be trained, such as 'lr_left', 'ud_axial', etc.
        set_no (str):
            the set number of the training data, such as 'set1', 'set2', etc.
        col (int):
            the number of features to be preserved, which is the number of columns in the psd spectrum.
        stats (bool):
            whether to add additional features such as mean and std of specific features.
            If True, the feature number will be increased by 2.
        model_save_path (str):
            the path to save the trained model. If None, the model will be saved at local directory.
        high_resolution (bool):
            whether the psd spectrum is high resolution, which will affect the model file name.
    Examples:
        >>> train_models(dataset_name='DS_L_01,PP_HR_400', dir='../../test_data//psd_20%//psd_window_high_resolution_20%//', 
                    format='parquet',
                    channel=channel, set_no=set_no, col=400, stats=False, 
                    model_save_path='../../model//20duty_high_resolution//', high_resolution=True,
                    ensemble_kwargs = {'ensemble_size': 5}, ensemble_nbest=10, metric=autosklearn.metrics.f1_weighted
                    )
    '''
    df = load_data(format=format, dir=dir, keyword=channel)
    df = preprocess_features(df, col=col, stats=stats)

    if df.isna().any().any():
        raise ValueError('nan values exist in the teat data.')
    
    X_train, X_test, y_train, y_test = train_test_split(df, test_samples=test_sample['set%d'%set_no])
    print(X_test.columns)
    
    if high_resolution:
        model_save_path += channel + '_high_resolution_' + 'set' + str(set_no) + '_' + str(col)
    else:
        model_save_path += channel + '_set' + str(set_no) + '_' + str(col)
    
    train_autosklearn_v1_model(dataset_name, model_save_path + '_v1.joblib', set_no, channel,  X_train, X_test, y_train, y_test, 
                               ensemble_kwargs = {'ensemble_size': 5}, ensemble_nbest=10, 
                               metric=metrics.CLASSIFICATION_METRICS['f1_weighted'])
    # use default metric for autosklearn v2, because f1_weighted is not supported in autosklearn v2
    train_autosklearn_v2_model(dataset_name, model_save_path + '_v2.joblib', set_no, channel, X_train, X_test, y_train, y_test,
                               ensemble_kwargs = {'ensemble_size': 5, 'metrics': metrics.CLASSIFICATION_METRICS['f1_weighted']},
                               ensemble_nbest=10)
    
if __name__ == '__main__':
    keyword = 'ud_axial'
    set_no = 3
    col = 400
    dir_model = '../../model//100duty_high_resolution_5ensemble//'
    '''
    # train models with autosklearn v1 and v2
    train_models(dataset_name='DS_H_01,PP_LR_%d_H'%col,dir='../../test_data//psd_100%//psd_window_100%//',
                  format='parquet',
                  channel=keyword, set_no=set_no, col=col, stats=False, model_save_path=dir_model, high_resolution=False)
    '''
    defact_list = ['a00053', 'a03720', 'a04802', 'a01833']
    def label_test(sample_num: str):
        if sample_num in ['b00053', 'b04802']:
            return 1
        else:
            return 0
    dir_psd_1 = '../../test_data//20250410_test_samples//psd_100%//psd_high_resolution.xlsx'
    dir_psd_2 = '../../test_data//20250613_test_samples//psd_100%_high_resolution.xlsx'
    df_1 = load_data(window=False, format='excel', dir=dir_psd_1, keyword=keyword)
    df_1 = df_1.loc[[x not in defact_list for x in df_1['sample_num']]]
    df_2 = load_data(window=False, format='excel', dir=dir_psd_2, keyword=keyword)
    df = pd.concat([df_1, df_2], axis=0).reset_index(drop=True)
    
    versions = ['v1', 'v2']
    # predict the test samples with the trained models
    for version in versions:
        joblib = dir_model + '%s_high_resolution_set%d_%d_%s.joblib'%(keyword, set_no, col, version)
        model_info(joblib, show_models=False)
        predict(df=df, psd_file='../../test_data//20250410_test_samples//psd_100%//psd.xlsx',
                joblib=joblib, keyword=keyword, col=col, stats=False, label_method=label_test) 