import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import signal_processing
from typing import Literal
import time

def shapley_value(x_train:np.ndarray, x_test:np.ndarray, y_train:np.ndarray, y_test:np.ndarray, X:pd.DataFrame):
    from sklearn.metrics import accuracy_score, mean_squared_error
    import shap
    from xgboost.sklearn import XGBRegressor

    shap.initjs()
    xgb_model = XGBRegressor(n_estimators=1000, max_depth=10, learning_rate=0.001, random_state=0)
    xgb_model.fit(x_train, y_train)
    y_predict = xgb_model.predict(x_test)
    print(y_predict)
    mean_squared_error(y_test, y_predict)**(0.5)
    explainer = shap.TreeExplainer(xgb_model)
    shap_values = explainer.shap_values(x_train)
    shap.summary_plot(shap_values, features=x_train, feature_names=X.columns)
    acc = accuracy_score(y_test, y_predict)
    print(acc)

def fisher_score_show(x_train:np.ndarray, x_test:np.ndarray, y_train:np.ndarray, y_test:np.ndarray):
    from skfeature.function.similarity_based import fisher_score
    from sklearn import svm
    from sklearn.metrics import accuracy_score

    score = fisher_score.fisher_score(x_train, y_train)
    idx = fisher_score.feature_ranking(score)
    num_fea = 5
    
    plt.figure(layout="constrained")
    plt.bar(range(len(score)), score)
    plt.xlabel('Feature Index')
    plt.ylabel('Fisher Score')
    plt.title('Fisher Score for Each Feature')
    for i in range(num_fea):
        plt.annotate(idx[i], 
                     xy=(idx[i], score[idx[i]]), rotation=0, xycoords='data',
                     xytext=(0,0), textcoords='offset pixels')
    plt.show()
    
    selected_feature_train = x_train[:, idx[0:num_fea]]
    selected_feature_test = x_test[:, idx[0:num_fea]]
    clf = svm.LinearSVC(dual=False)
    clf.fit(selected_feature_train, y_train)
    y_predict = clf.predict(selected_feature_test)
    acc = accuracy_score(y_test, y_predict)
    print(acc)

def compare_target_predict(df:pd.DataFrame, target:str, predict:str):
    '''
    show wrong prediction spectrum

    Parameters
    ----------
    df : input dataframe
    target : str
        name pf target column
    predict : str
        name of predict column
    
    Examples
    --------
    >>> compare_target_predict(df, 'Type', 'Predict')
    '''
    df_wrong = df.loc[df[target] != df[predict], :]
    fig, axs = plt.subplots(1,1, layout='constrained')
    for i in df_wrong.index:
        axs.plot(df_wrong.loc[i][:151], label= i + ': target %i predict %i'%(df_wrong.loc[i, 'Type'], df_wrong.loc[i, 'Predict']))
    axs.set_xticks(np.arange(0,151, step=10))
    axs.set_yscale('log')
    axs.legend()
    plt.show()

def predict(psd_file:str, joblib:str, keyword:str, col:int, column:list):
    '''
    load psd spectrum, and load model, output the predict result

    Parameters
    ----------
    psd_file : str
            psd spectrum generated by `acc_processing_excel()`. The samples are saved in separated sheets, 
            the column label in each sheets is the sensor channel, and the index is the order number.
            The psd result is averaged by windows.
    joblib : str
            file name of the trained classification model
    keyword : str
            selected sensor channel
    col : int
            controls the feature number to match the model feature number
    column : list
            controls the column label to match the model feature labels
    
    Examples
    --------
    >>> X = signal_processing.read_parquet_keyword(
            'lr_left',
            dir = '../../test_data//psd_100%//psd_window_high_resolution_100%//', 
            parse_func=signal_processing.parse_digital).sample(frac=1).reset_index(drop=True)
        predict(psd_file='../../test_data//20250410_test_samples//psd_100%//psd_high_resolution.xlsx', 
                joblib='../../model//100%_high_resolution//lr_left_high_resolution_set1_1280_v1.joblib',
                keyword='lr_left', col=1280, column=X.columns)
    '''
    from joblib import load
    df = signal_processing.read_sheets(psd_file, usecols=[0,1,2,3], combine=True)
    df = df.transpose()
    df.rename(columns=float, inplace=True)
    # add stats
    df = signal_processing.calculate_spectral_stats(30,80,df)

    # add columns to describe the sensor channel and the sample_num
    df['channel'] = [name[7:] for name in df.index]
    df['sample_num'] = [name[:6] for name in df.index]

    # select a particular channel and shuffle
    X_test = df.loc[df['channel'] == keyword]
    X_test = signal_processing.cast_column_to_str(X_test.drop(columns=['channel']), 1, labels=column)
    X_test = pd.concat([X_test.iloc[:, :col], X_test.iloc[:, 129], X_test.iloc[:, 130]], axis=1)
    if X_test.isna().any().any():
        raise ValueError('nan values exist in the teat data.')
    
    # 匯入模型
    clf = load(joblib)
    # 模型預測測試
    res = df.loc[df['channel'] == keyword].iloc[:, -1:].set_index('sample_num')
    res['predict'] = clf.predict(X_test)
    print(joblib.split('/')[-1][:-7])
    print(res)

def load_data(format:Literal['excel', 'parquet'], dir:str, keyword:str):
    '''
    load data of chosen format and filtered with keyword.

    Parameters
    ----------
    format : psd spectrum saving format
    dir : for excel file, the file name of the spectrum. And for parquet file, the directory.
    keyword : the channel of sensor
    '''
    if format == 'excel':
        df = signal_processing.read_sheets(filename=dir, combine=True, axis=0)
        # add columns to describe the sensor channel and the sample_num
        df['channel'] = [name[7:] for name in df['name']]
        df['sample_num'] = [name[:6] for name in df['name']]
        # select a particular channel and shuffle
        df = df.loc[df['channel'] == keyword].drop(column='channel').sample(frac = 1).reset_index(drop=True)
    
    else:
        # select a particular channel and shuffle
        df = signal_processing.read_parquet_keyword(keyword, dir, 
                                                parse_func=signal_processing.parse_digital).sample(frac=1).reset_index(drop=True)
    if df.isna().any().any():
        raise ValueError('nan values exist in the teat data.')
    return df
    
    
def preprocess_features(df:pd.DataFrame, col:int, stats:bool=False):
    '''
    preprocessing

    Parameters
    ----------
    df : dataframe contains 'sample_num' 
    col : preserved column number
    stats : bool
        whether to add additional features such as mean and std of specific features
    '''
    # add additional features such as mean and std of specific features
    if stats:
        df = signal_processing.calculate_spectral_stats(30, 80, df)
        # drop unused columns to reduce feature number
        df = pd.concat([df.iloc[:, :col], df.iloc[:, -3:]], axis=1)
    else:
        df = pd.concat([df.iloc[:, :col], df['sample_num']], axis=1)
    # transfer the column label into string before training
    df = signal_processing.cast_column_to_str(df, 2)
    return df

def train_test_split(df:pd.DataFrame, test_samples: list):
    '''
    separate train and test set

    Parameters
    ----------
    df : dataframe contains 'sample_num'
    '''
    X_train = df.loc[[x not in test_samples for x in df['sample_num']]]
    X_test = df.loc[[x in test_samples for x in df['sample_num']]]

    y_train = np.array([label_transfer(sample_num) for sample_num in X_train['sample_num']])
    y_test = np.array([label_transfer(sample_num) for sample_num in X_test['sample_num']])

    # encode categorical columns
    y_train = pd.DataFrame(y_train, dtype="category")
    y_test = pd.DataFrame(y_test, dtype="category")

    # drop target
    X_train = X_train.drop(columns='sample_num').reset_index(drop=True)
    X_test = X_test.drop(columns='sample_num').reset_index(drop=True)
    print('train shape:', X_train.shape)
    print('test shape:', X_test.shape)
    return X_train, X_test, y_train, y_test

test_sample = {
    'set1' : ['000027', '000048', '000053', '003735', '004073', '000785'],
    'set2' : ['000030', '000050', '003735', '003861', '001833', '002577'],
    'set3' : ['000030', '000039', '000052', '004072', '004073', '004802']
}

def label_transfer(sample_num: str):
    '''
    transfer sample number to label 0 and 1
    '''
    return 0 if signal_processing.class_label(sample_num) == 0 else 1


def model_info():
    '''
    print structured model info for convenience pasting on table
    reference: training data summery table
    '''
    pass

def train_autosklearn_v1_model(X_train, X_test, y_train, y_test, time_limit = 600, per_run_limit = 200, n_jobs = -1):
    import autosklearn.classification
    automlclassifierV1 = autosklearn.classification.AutoSklearnClassifier(
        time_left_for_this_task=time_limit,
        per_run_time_limit=per_run_limit,
        resampling_strategy='cv',
        resampling_strategy_arguments={'folds': 5},
        memory_limit=None,
        n_jobs=n_jobs
    )
    t1 = time.time()
    automlclassifierV1.fit(X_train, y_train)
    t2 = time.time()
    print(f'autosklearnV1 training time: {t2 - t1:.2f}')
    # print score
    print('automlclassifierV1 訓練集: ',automlclassifierV1.score(X_train,y_train))
    print('automlclassifierV1 測試集: ',automlclassifierV1.score(X_test,y_test))
    return automlclassifierV1
    
def train_autosklearn_v2_model(X_train, X_test, y_train, y_test, time_limit = 600, per_run_limit = 200, n_jobs = -1):
    from autosklearn.experimental.askl2 import AutoSklearn2Classifier

    automlclassifierV2 = AutoSklearn2Classifier(
        time_left_for_this_task=time_limit,
        per_run_time_limit=per_run_limit,
        memory_limit=None,
        n_jobs=n_jobs
    )
    t1 = time.time()
    automlclassifierV2.fit(X_train, y_train)
    t2 = time.time()
    print(f'autosklearnV2 training time: {t2 - t1:.2f}')
    # print score    
    print('automlclassifierV2 訓練集: ',automlclassifierV2.score(X_train,y_train))
    print('automlclassifierV2 測試集: ',automlclassifierV2.score(X_test,y_test))
    return automlclassifierV2

def save_model(automlclassifierV1, automlclassifierV2, path:str):
    from joblib import dump
    dump(automlclassifierV1, path + '_v1.joblib')
    dump(automlclassifierV2, path + '_v2.joblib')


def train_models(dir:str, channel:str, set_no:str, col:int, stats:bool=False, model_save_path:str=None, high_resolution:bool=False):
    '''
    train models with autosklearn v1 and v2

    Parameters
    ----------
    dir : str
        directory of the psd spectrum
    channel : str
        sensor channel
    set_no : str
        test sample set number
    col : int
        preserved column number
    stats : bool
        whether to add additional features such as mean and std of specific features
    Examples
    --------
    >>> train_models(dir='../../test_data//psd_20%//psd_window_high_resolution_20%//', 
                channel=channel, set_no=set_no, col=400, stats=False, 
                model_save_path='../../model//20duty_high_resolution//', high_resolution=True)
    '''
    df = load_data(format='parquet', dir=dir, keyword=channel)
    df = preprocess_features(df, col=col, stats=stats)

    if df.isna().any().any():
        raise ValueError('nan values exist in the teat data.')
    
    X_train, X_test, y_train, y_test = train_test_split(df, test_samples=test_sample[set_no])
    print(X_test.columns)
    automlclassifierV1 = train_autosklearn_v1_model(X_train, X_test, y_train, y_test)
    automlclassifierV2 = train_autosklearn_v2_model(X_train, X_test, y_train, y_test)
    
    if high_resolution:
        model_save_path += channel + '_high_resolution_' + set_no + '_' + str(col)
    else:
        model_save_path += channel + set_no + '_' + str(col)
    
    save_model(automlclassifierV1, automlclassifierV2, path=model_save_path)
    print('model saved at:', model_save_path)

if __name__ == '__main__':
    channel = 'ud_up'
    set_no = 'set1'
    
    train_models(dir='../../test_data//psd_20%//psd_window_high_resolution_20%//', 
                channel=channel, set_no=set_no, col=400, stats=False, 
                model_save_path='../../model//20duty_high_resolution//', high_resolution=True)