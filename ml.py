import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import signal_processing
from typing import Literal
import time

def shapley_value(x_train:np.ndarray, x_test:np.ndarray, y_train:np.ndarray, y_test:np.ndarray, X:pd.DataFrame):
    from sklearn.metrics import accuracy_score, mean_squared_error
    import shap
    from xgboost.sklearn import XGBRegressor

    shap.initjs()
    xgb_model = XGBRegressor(n_estimators=1000, max_depth=10, learning_rate=0.001, random_state=0)
    xgb_model.fit(x_train, y_train)
    y_predict = xgb_model.predict(x_test)
    print(y_predict)
    mean_squared_error(y_test, y_predict)**(0.5)
    explainer = shap.TreeExplainer(xgb_model)
    shap_values = explainer.shap_values(x_train)
    shap.summary_plot(shap_values, features=x_train, feature_names=X.columns)
    acc = accuracy_score(y_test, y_predict)
    print(acc)

def fisher_score_show(x_train:np.ndarray, x_test:np.ndarray, y_train:np.ndarray, y_test:np.ndarray):
    from skfeature.function.similarity_based import fisher_score
    from sklearn import svm
    from sklearn.metrics import accuracy_score

    score = fisher_score.fisher_score(x_train, y_train)
    idx = fisher_score.feature_ranking(score)
    num_fea = 5
    
    plt.figure(layout="constrained")
    plt.bar(range(len(score)), score)
    plt.xlabel('Feature Index')
    plt.ylabel('Fisher Score')
    plt.title('Fisher Score for Each Feature')
    for i in range(num_fea):
        plt.annotate(idx[i], 
                     xy=(idx[i], score[idx[i]]), rotation=0, xycoords='data',
                     xytext=(0,0), textcoords='offset pixels')
    plt.show()
    
    selected_feature_train = x_train[:, idx[0:num_fea]]
    selected_feature_test = x_test[:, idx[0:num_fea]]
    clf = svm.LinearSVC(dual=False)
    clf.fit(selected_feature_train, y_train)
    y_predict = clf.predict(selected_feature_test)
    acc = accuracy_score(y_test, y_predict)
    print(acc)

def compare_target_predict(df:pd.DataFrame, target:str, predict:str):
    '''
    show wrong prediction spectrum

    Parameters
    ----------
    df : input dataframe
    target : str
        name pf target column
    predict : str
        name of predict column
    
    Examples
    --------
    >>> compare_target_predict(df, 'Type', 'Predict')
    '''
    df_wrong = df.loc[df[target] != df[predict], :]
    fig, axs = plt.subplots(1,1, layout='constrained')
    for i in df_wrong.index:
        axs.plot(df_wrong.loc[i][:151], label= i + ': target %i predict %i'%(df_wrong.loc[i, 'Type'], df_wrong.loc[i, 'Predict']))
    axs.set_xticks(np.arange(0,151, step=10))
    axs.set_yscale('log')
    axs.legend()
    plt.show()

# (Moved above to fix default argument error)
def label_transfer(sample_num: str):
    '''
    transfer sample number to label 0 and 1
    '''
    return 0 if signal_processing.class_label(sample_num) == 0 else 1

def predict(psd_file:str, joblib:str, keyword:str, col:int, stats:bool, label_method = label_transfer):
    '''
    load psd spectrum, and load model, output the predict result

    Args:
        psd_file(str):
            psd spectrum generated by `acc_processing_excel()`. The samples are saved in separated sheets, 
            the column label in each sheets is the sensor channel, and the index is the order number.
            The psd result is averaged by windows.
        joblib(str): 
            file name of the trained classification model
        keyword(str):
            selected sensor channel
        col(int):
            controls the feature number to match the model feature number
        stats(bool):
            whether to add additional features such as mean and std of specific features.
            If True, the feature number will be increased by 2.
    Examples:
        >>> predict(psd_file='../../test_data//20250410_test_samples//psd_20%//psd_high_resolution.xlsx',
                    joblib='../../model//20duty_high_resolution//lr_left_high_resolution_set1_5121_v1.joblib',
                    keyword='lr_left', col=5121, stats=False)
    '''
    # 匯入模型
    from joblib import load
    from sklearn.metrics import classification_report
    clf = load(joblib)
    
    df = load_data(window=False, format='excel', dir=psd_file, keyword=keyword)

    res = df['sample_num'].to_frame()
    res['label'] = df['sample_num'].apply(label_method)

    # 模型預測測試
    t1 = time.time()
    df = preprocess_features(df, col=col, stats=stats)
    res['predict'] = clf.predict(df.drop(columns=['sample_num']))
    t2 = time.time()
    total_inference_time_ms = (t2 - t1) * 1000
    
    print(joblib.split('/')[-1][:-7])
    print(res)
    print(classification_report(res['label'], res['predict']))
    print(f'Total inference time: {total_inference_time_ms:.2f} ms')
        
def predict_single(psd_file:str, joblib:str, keyword:str, col:int, stats:bool, label_method = label_transfer):
    # 匯入模型
    from joblib import load
    from sklearn.metrics import classification_report

    clf = load(joblib)
    
    df = load_data(window=False, format='excel', dir=psd_file, keyword=keyword)

    total_samples = df['sample_num'].shape[0]
    total_inference_time_ms = 0
    res = df['sample_num'].to_frame()
    res['label'] = df['sample_num'].apply(label_method)

    for i in range(total_samples):
        single_sample = df.iloc[i, :].to_frame().transpose()
        # 模型預測測試
        t1 = time.time()
        single_sample = preprocess_features(single_sample, col=col, stats=stats)
        target = clf.predict(single_sample.drop(columns=['sample_num']))
        t2 = time.time()
        total_inference_time_ms += (t2 - t1) * 1000
        res.loc[i, 'predict'] = target
    
    print(joblib.split('/')[-1][:-7])
    print(res)
    print(classification_report(res['label'], res['predict']))
    print(f'Total inference time: {total_inference_time_ms:.2f} ms')
    print(f'Average inference time per sample: {total_inference_time_ms / total_samples:.2f} ms')
    
def load_data(format:Literal['excel', 'parquet'], dir:str, keyword:str, window:bool = True):
    '''
    load data of chosen format and filtered with keyword.
    Args:
        format (Literal['excel', 'parquet']):
            the format of the data, either 'excel' or 'parquet', which contains the psd spectrum.
        dir (str): the directory of the parquet file or the filename of the excel file.
        keyword (str): the channel of sensor, such as 'lr_left', 'lr_right', etc.
        window (bool): whether the data is windowed, default is True. If True, the data is read from un-averaged psd spectrum.
        
    Returns:
        pd.DataFrame: a dataframe containing the psd spectrum data. The columns are the order number of the psd spectrum,
        the sensor channel, and the sample number. The index is shuffled.
    Examples:
        >>> df = load_data(format='parquet', dir='../../test_data//psd_20%//psd_window_high_resolution_20%//', keyword='lr_left')
        >>> df.head()
    Raises:
        ValueError: if the dataframe contains NaN values.
    '''
    if format == 'excel':
        df = pd.DataFrame()
        if not window:
            df = signal_processing.read_sheets(filename=dir, usecols=[0,1,2,3], combine=True)
            df = df.transpose()
            df['name'] = df.index
            df = df.reset_index(drop=True)
        
        else:
            df = signal_processing.read_sheets(filename=dir, combine=True, axis=0)
        
        # add columns to describe the sensor channel and the sample_num
        df['channel'] = [name[7:] for name in df['name']]
        df['sample_num'] = [name[:6] for name in df['name']]
        # select a particular channel and shuffle
        df = df.loc[df['channel'] == keyword].drop(columns='channel').sample(frac = 1).reset_index(drop=True)
    
    else:
        # select a particular channel and shuffle
        df = signal_processing.read_parquet_keyword(keyword, dir, 
                                                parse_func=signal_processing.parse_digital).sample(frac=1).reset_index(drop=True)
    if df.isna().any().any():
        raise ValueError('nan values exist in the teat data.')
    return df
    
    
def preprocess_features(df:pd.DataFrame, col:int, stats:bool=False):
    '''
    preprocessing

    Parameters
    ----------
    df : dataframe contains 'sample_num' 
    col : preserved column number
    stats : bool
        whether to add additional features such as mean and std of specific features
    '''
    # add additional features such as mean and std of specific features
    if stats:
        df = signal_processing.calculate_spectral_stats(30, 80, df)
        # drop unused columns to reduce feature number
        df = pd.concat([df.iloc[:, :col], df.iloc[:, -3:]], axis=1)
    else:
        df = pd.concat([df.iloc[:, :col], df['sample_num']], axis=1)
    # transfer the column label into string before training
    df = signal_processing.cast_column_to_str(df, 2)
    if df.isna().any().any():
        raise ValueError('nan values exist in the teat data.')
        
    return df

def train_test_split(df:pd.DataFrame, test_samples: list):
    '''
    separate train and test set

    Parameters
    ----------
    df : dataframe contains 'sample_num'
    '''
    X_train = df.loc[[x not in test_samples for x in df['sample_num']]]
    X_test = df.loc[[x in test_samples for x in df['sample_num']]]

    y_train = np.array([label_transfer(sample_num) for sample_num in X_train['sample_num']])
    y_test = np.array([label_transfer(sample_num) for sample_num in X_test['sample_num']])

    # encode categorical columns
    y_train = pd.DataFrame(y_train, dtype="category")
    y_test = pd.DataFrame(y_test, dtype="category")

    # drop target
    X_train = X_train.drop(columns='sample_num').reset_index(drop=True)
    X_test = X_test.drop(columns='sample_num').reset_index(drop=True)
    print('train shape:', X_train.shape)
    print('test shape:', X_test.shape)
    return X_train, X_test, y_train, y_test

def model_info():
    '''
    print structured model info for convenience pasting on table
    reference: training data summery table
    '''
    pass

def train_autosklearn_v1_model(X_train, X_test, y_train, y_test, time_limit = 600, per_run_limit = 200, n_jobs = -1):
    import autosklearn.classification
    automlclassifierV1 = autosklearn.classification.AutoSklearnClassifier(
        time_left_for_this_task=time_limit,
        per_run_time_limit=per_run_limit,
        resampling_strategy='cv',
        resampling_strategy_arguments={'folds': 5},
        memory_limit=None,
        n_jobs=n_jobs
    )
    t1 = time.time()
    automlclassifierV1.fit(X_train, y_train)
    t2 = time.time()
    print(f'autosklearnV1 training time: {t2 - t1:.2f}')
    # print score
    print('automlclassifierV1 訓練集: ',automlclassifierV1.score(X_train,y_train))
    print('automlclassifierV1 測試集: ',automlclassifierV1.score(X_test,y_test))
    return automlclassifierV1
    
def train_autosklearn_v2_model(X_train, X_test, y_train, y_test, time_limit = 600, per_run_limit = 200, n_jobs = -1):
    from autosklearn.experimental.askl2 import AutoSklearn2Classifier

    automlclassifierV2 = AutoSklearn2Classifier(
        time_left_for_this_task=time_limit,
        per_run_time_limit=per_run_limit,
        memory_limit=None,
        n_jobs=n_jobs
    )
    t1 = time.time()
    automlclassifierV2.fit(X_train, y_train)
    t2 = time.time()
    print(f'autosklearnV2 training time: {t2 - t1:.2f}')
    # print score    
    print('automlclassifierV2 訓練集: ',automlclassifierV2.score(X_train,y_train))
    print('automlclassifierV2 測試集: ',automlclassifierV2.score(X_test,y_test))
    return automlclassifierV2

def save_model(automlclassifierV1, automlclassifierV2, path:str):
    from joblib import dump
    dump(automlclassifierV1, path + '_v1.joblib')
    dump(automlclassifierV2, path + '_v2.joblib')


def train_models(dir:str, channel:str, set_no:str, col:int, stats:bool=False, model_save_path:str=None, high_resolution:bool=False):
    '''
    train models with autosklearn v1 and v2

    Parameters
    ----------
    dir : str
        directory of the psd spectrum
    channel : str
        sensor channel
    set_no : str
        test sample set number
    col : int
        preserved column number
    stats : bool
        whether to add additional features such as mean and std of specific features
    Examples
    --------
    >>> train_models(dir='../../test_data//psd_20%//psd_window_high_resolution_20%//', 
                channel=channel, set_no=set_no, col=400, stats=False, 
                model_save_path='../../model//20duty_high_resolution//', high_resolution=True)
    '''
    df = load_data(format='parquet', dir=dir, keyword=channel)
    df = preprocess_features(df, col=col, stats=stats)

    if df.isna().any().any():
        raise ValueError('nan values exist in the teat data.')
    
    X_train, X_test, y_train, y_test = train_test_split(df, test_samples=test_sample[set_no])
    print(X_test.columns)
    automlclassifierV1 = train_autosklearn_v1_model(X_train, X_test, y_train, y_test)
    automlclassifierV2 = train_autosklearn_v2_model(X_train, X_test, y_train, y_test)
    
    if high_resolution:
        model_save_path += channel + '_high_resolution_' + set_no + '_' + str(col)
    else:
        model_save_path += channel + set_no + '_' + str(col)
    
    save_model(automlclassifierV1, automlclassifierV2, path=model_save_path)
    print('model saved at:', model_save_path)

if __name__ == '__main__':
    keyword = 'lr_left'
    col = 5121
    set_no = 'set1'
    joblib = '../../model//20duty_high_resolution//%s_high_resolution_%s_%d_v1.joblib'%(keyword, set_no, col)
    def label(sample_num: str):
        if sample_num in ['a00053', 'a03720', 'a04802', 'a01833']:
            return 1
        else:
            return 0
    
    predict_single(psd_file='../../test_data//20250410_test_samples//psd_20%//psd_high_resolution.xlsx', stats=False,
            joblib=joblib, keyword=keyword, col=col, label_method=label)
    