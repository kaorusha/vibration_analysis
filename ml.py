import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import signal_processing
from typing import Literal, Union
import time
from joblib import load
from sklearn.metrics import classification_report
import autosklearn.metrics as metrics
import mlflow
import mlflow.sklearn

def shapley_value(x_train:np.ndarray, x_test:np.ndarray, y_train:np.ndarray, y_test:np.ndarray, X:pd.DataFrame):
    from sklearn.metrics import accuracy_score, mean_squared_error
    import shap
    from xgboost.sklearn import XGBRegressor

    shap.initjs()
    xgb_model = XGBRegressor(n_estimators=1000, max_depth=10, learning_rate=0.001, random_state=0)
    xgb_model.fit(x_train, y_train)
    y_predict = xgb_model.predict(x_test)
    print(y_predict)
    mean_squared_error(y_test, y_predict)**(0.5)
    explainer = shap.TreeExplainer(xgb_model)
    shap_values = explainer.shap_values(x_train)
    shap.summary_plot(shap_values, features=x_train, feature_names=X.columns)
    acc = accuracy_score(y_test, y_predict)
    print(acc)

def fisher_score_show(x_train:np.ndarray, x_test:np.ndarray, y_train:np.ndarray, y_test:np.ndarray):
    from skfeature.function.similarity_based import fisher_score
    from sklearn import svm
    from sklearn.metrics import accuracy_score

    score = fisher_score.fisher_score(x_train, y_train)
    idx = fisher_score.feature_ranking(score)
    num_fea = 5
    
    plt.figure(layout="constrained")
    plt.bar(range(len(score)), score)
    plt.xlabel('Feature Index')
    plt.ylabel('Fisher Score')
    plt.title('Fisher Score for Each Feature')
    for i in range(num_fea):
        plt.annotate(idx[i], 
                     xy=(idx[i], score[idx[i]]), rotation=0, xycoords='data',
                     xytext=(0,0), textcoords='offset pixels')
    plt.show()
    
    selected_feature_train = x_train[:, idx[0:num_fea]]
    selected_feature_test = x_test[:, idx[0:num_fea]]
    clf = svm.LinearSVC(dual=False)
    clf.fit(selected_feature_train, y_train)
    y_predict = clf.predict(selected_feature_test)
    acc = accuracy_score(y_test, y_predict)
    print(acc)

def compare_target_predict(df:pd.DataFrame, target:str, predict:str):
    '''
    show wrong prediction spectrum

    Parameters
    ----------
    df : input dataframe
    target : str
        name pf target column
    predict : str
        name of predict column
    
    Examples
    --------
    >>> compare_target_predict(df, 'Type', 'Predict')
    '''
    df_wrong = df.loc[df[target] != df[predict], :]
    fig, axs = plt.subplots(1,1, layout='constrained')
    for i in df_wrong.index:
        axs.plot(df_wrong.loc[i][:151], label= i + ': target %i predict %i'%(df_wrong.loc[i, 'Type'], df_wrong.loc[i, 'Predict']))
    axs.set_xticks(np.arange(0,151, step=10))
    axs.set_yscale('log')
    axs.legend()
    plt.show()

# (Moved above to fix default argument error)
def label_transfer(sample_num: str):
    '''
    transfer sample number to label 0 and 1
    '''
    return 0 if signal_processing.class_label(sample_num) == 0 else 1

def predict(psd_file:str, joblib:str, keyword:str, col:int, stats:bool, label_method = label_transfer, df=None):
    '''
    load psd spectrum, and load model, output the predict result

    Args:
        df(pd.DataFrame):
            dataframe containing the psd spectrum data. The columns are the order number of the psd spectrum,
            the sensor channel, and the sample number. The index is shuffled.
            If None, the data will be loaded from `psd_file`.
        psd_file(str):
            psd spectrum generated by `acc_processing_excel()`. The samples are saved in separated sheets, 
            the column label in each sheets is the sensor channel, and the index is the order number.
            The psd result is averaged by windows.
        joblib(str): 
            file name of the trained classification model
        keyword(str):
            selected sensor channel
        col(int):
            controls the feature number to match the model feature number
        stats(bool):
            whether to add additional features such as mean and std of specific features.
            If True, the feature number will be increased by 2.
    Examples:
        >>> predict(psd_file='../../test_data//20250410_test_samples//psd_20%//psd_high_resolution.xlsx',
                    joblib='../../model//20duty_high_resolution//lr_left_high_resolution_set1_5121_v1.joblib',
                    keyword='lr_left', col=5121, stats=False)
    '''
    # 匯入模型
    clf = load(joblib)
    if df is None:
        df = load_data(window=False, format='excel', dir=psd_file, keyword=keyword)

    res = df['sample_num'].to_frame()
    res['label'] = df['sample_num'].apply(label_method)

    # 模型預測測試
    t1 = time.time()
    df = preprocess_features(df, col=col, stats=stats)
    res['predict'] = clf.predict(df.drop(columns=['sample_num']))
    t2 = time.time()
    total_inference_time_ms = (t2 - t1) * 1000
    
    print(joblib.split('/')[-1][:-7])
    print(res)
    print(classification_report(res['label'], res['predict']))
    print(f'Total inference time: {total_inference_time_ms:.2f} ms')
        
def predict_single(psd_file:str, joblib:str, keyword:str, col:int, stats:bool, label_method = label_transfer):
    # 匯入模型
    clf = load(joblib)
    
    df = load_data(window=False, format='excel', dir=psd_file, keyword=keyword)

    total_samples = df['sample_num'].shape[0]
    total_inference_time_ms = 0
    res = df['sample_num'].to_frame()
    res['label'] = df['sample_num'].apply(label_method)

    for i in range(total_samples):
        single_sample = df.iloc[i, :].to_frame().transpose()
        # 模型預測測試
        t1 = time.time()
        single_sample = preprocess_features(single_sample, col=col, stats=stats)
        target = clf.predict(single_sample.drop(columns=['sample_num']))
        t2 = time.time()
        total_inference_time_ms += (t2 - t1) * 1000
        res.loc[i, 'predict'] = target
    
    print(joblib.split('/')[-1][:-7])
    print(res)
    print(classification_report(res['label'], res['predict']))
    print(f'Total inference time: {total_inference_time_ms:.2f} ms')
    print(f'Average inference time per sample: {total_inference_time_ms / total_samples:.2f} ms')
    
def load_data(format:Literal['excel', 'parquet'], dir:str, keyword:str, window:bool = True,
              parse_func = signal_processing.parse_digital):
    '''
    load data of chosen format and filtered with keyword.
    Args:
        format (Literal['excel', 'parquet']):
            the format of the data, either 'excel' or 'parquet', which contains the psd spectrum.
        dir (str): the directory of the parquet file or the filename of the excel file.
        keyword (str): the channel of sensor, such as 'lr_left', 'lr_right', etc.
        window (bool): whether the data is windowed, default is True. If True, the data is read from un-averaged psd spectrum.
        
    Returns:
        pd.DataFrame: a dataframe containing the psd spectrum data. The columns are the order number of the psd spectrum,
        the sensor channel, and the sample number. The index is shuffled.
    Examples:
        >>> df = load_data(format='parquet', dir='../../test_data//psd_20%//psd_window_high_resolution_20%//', keyword='lr_left')
        >>> df.head()
    Raises:
        ValueError: if the dataframe contains NaN values.
    '''
    if format == 'excel':
        df = pd.DataFrame()
        if not window:
            df = signal_processing.read_sheets(filename=dir, usecols=[0,1,2,3], combine=True)
            df = df.transpose()
            df['name'] = df.index
            df = df.reset_index(drop=True)
        
        else:
            df = signal_processing.read_sheets(filename=dir, combine=True, axis=0)
        
        # add columns to describe the sensor channel and the sample_num
        df['channel'] = [name[7:] for name in df['name']]
        df['sample_num'] = [name[:6] for name in df['name']]
        # select a particular channel and shuffle
        df = df.loc[df['channel'] == keyword].drop(columns='channel').sample(frac = 1).reset_index(drop=True)
    
    else:
        # select a particular channel and shuffle
        df = signal_processing.read_parquet_keyword(keyword, dir, 
                                                parse_func=parse_func).sample(frac=1).reset_index(drop=True)
    if df.isna().any().any():
        raise ValueError('nan values exist in the teat data.')
    return df
    
    
def preprocess_features(df:pd.DataFrame, col:Union[int, list, tuple], stats:bool=False):
    '''
    preprocessing

    Parameters
    ----------
    df : dataframe contains 'sample_num' 
    col : int, list or tuple (lwr, upr)
        If int: preserve the first `col` columns.
        If list: preserve the columns specified by the list.
        If tuple: a range (lwr, upr) to preserve columns from lwr to upr (exclusive).
    stats : bool
        whether to add additional features such as mean and std of specific features
    '''
    if not isinstance(col, (int, list, tuple)):
        raise TypeError('col must be int, list or tuple, but got %s.' % type(col))
    # check if the column number is valid
    if isinstance(col, int):
        if col <= 0 or col > df.shape[1] - 1:
            raise ValueError('col must be a positive integer less than the number of columns in the dataframe, '
                             'but got %d.' % col)
        preserve_df = df.iloc[:, :col]
    elif isinstance(col, (list, tuple)):
        if len(col) == 0:
            raise ValueError('col must be a non-empty list or tuple, but got an empty %s.' % type(col))
        # check if the column number is valid
        if not all(isinstance(i, int) and 0 <= i < df.shape[1] for i in col):
            raise ValueError('col must be a list or tuple of positive integers less than the number of columns in the dataframe, '
                             'but got %s.' % col)
        if isinstance(col, list):
            if len(col) == 1:
                preserve_df = df.iloc[:, col[0]]
            else:
                preserve_df = df.iloc[:, col]
        else:  # tuple
            if len(col) != 2:
                raise ValueError('col must be a tuple of two integers, but got %s.' % col)
            if col[0] < 0 or col[1] > df.shape[1] or col[0] >= col[1]:
                raise ValueError('col must be a tuple of two integers, where the first integer is less than the second integer, '
                                 'and both integers are less than the number of columns in the dataframe, but got %s.' % col)
            preserve_df = df.iloc[:, col[0]:col[1]]
    # add additional features such as mean and std of specific features
    if stats:
        df = signal_processing.calculate_spectral_stats(30, 80, df)
        # drop unused columns to reduce feature number
        df = pd.concat([preserve_df, df.iloc[:, -3:]], axis=1)
    else:
        df = pd.concat([preserve_df, df['sample_num']], axis=1)
    # transfer the column label into string before training
    df = signal_processing.cast_column_to_str(df, 2)
    if df.isna().any().any():
        raise ValueError('nan values exist in the teat data.')
        
    return df

def train_test_split(df:pd.DataFrame, test_samples: list, label_mothod = label_transfer):
    '''
    separate train and test set

    Parameters
    ----------
    df : dataframe contains 'sample_num'
    '''
    X_train = df.loc[[x not in test_samples for x in df['sample_num']]]
    X_test = df.loc[[x in test_samples for x in df['sample_num']]]

    y_train = np.array([label_mothod(sample_num) for sample_num in X_train['sample_num']])
    y_test = np.array([label_mothod(sample_num) for sample_num in X_test['sample_num']])

    # encode categorical columns
    y_train = pd.DataFrame(y_train, dtype="category")
    y_test = pd.DataFrame(y_test, dtype="category")

    # drop target
    X_train = X_train.drop(columns='sample_num').reset_index(drop=True)
    X_test = X_test.drop(columns='sample_num').reset_index(drop=True)
    print('train shape:', X_train.shape)
    print('test shape:', X_test.shape)
    return X_train, X_test, y_train, y_test

def model_info(model_file_name: str, leaderboard:bool = True, show_models:bool = True, sprint_statistics:bool = True):
    '''
    load a trained model and print model info
    Args:
        model_file_name (str): 
            the file name of the trained model, which is a joblib file.
        leaderboard (bool):
            whether to print the leaderboard of the model.
        show_models (bool): 
            whether to show the models in detail.
        sprint_stats (bool): 
            whether to print the sprint statistics of the model.
    '''
    model = load(model_file_name)
    
    if leaderboard:
        df = model.leaderboard(detailed = True, ensemble_only=True)
        print(df)
    # detail of the model
    if show_models:
        best_model_info = model.show_models()
        print(best_model_info)
    if sprint_statistics:
        print(model.sprint_statistics())
    
def model_training_log(model, model_file_name: str, X_train:pd.DataFrame, y_train:pd.DataFrame, X_test:pd.DataFrame, y_test:pd.DataFrame,
                       actual_training_time:float, dataset_name:str, train_test_split:int, channel:str):
    '''
    print model training log for convenience pasting on table
    reference: training data summery table
    Args:
        model: 
            the trained model, which is returned by fit().
        model_file_name (str): 
            the file name of the trained model, which is a joblib file.
        X_train (pd.DataFrame): 
            the training data features.
        y_train (pd.DataFrame): 
            the training data labels.
        X_test (pd.DataFrame): 
            the test data features.
        y_test (pd.DataFrame): 
            the test data labels.
        actual_training_time (float): 
            the actual training time of the model in seconds.
        dataset_name (str): 
            the name of the dataset, which will be used in the model training log and fit().
            the format is 'dataset_id,preprocessing_id', such as 'DS_L_01,PP_HR_400'.
        train_test_split (int): 
            the set number of the training data, such as 1, 2, etc.
        channel (str): 
            the sensor channel to be trained, such as 'lr_left', 'ud_axial', etc.
    '''
    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())
    train_report_dict = classification_report(y_train, model.predict(X_train), output_dict=True)
    test_report_dict = classification_report(y_test, model.predict(X_test), output_dict=True)
    dataset_id = dataset_name.split(',')[0] if dataset_name else 'unknown'
    preprocessing_id = dataset_name.split(',')[1] if len(dataset_name.split(',')) > 1 else 'unknown'

    print(
        f"{timestamp} | {model_file_name} | {model.__class__.__name__} | {model} | {actual_training_time:.2f} |"
        f"{dataset_id} | {preprocessing_id} | {train_test_split} |"
        f"{channel}| | {train_report_dict['accuracy']:.4f} |  | {test_report_dict['accuracy']:.4f} | "
        f"{test_report_dict['weighted avg']['precision']:.4f} | {test_report_dict['weighted avg']['recall']:.4f} | "
        f"{test_report_dict['weighted avg']['f1-score']:.4f} "
    )
    

def train_autosklearn_v1_model(dataset_name:str, model_path:str, set_no:int, channel:str, 
                               X_train, X_test, y_train, y_test, 
                               time_limit = 600, per_run_limit = 200, n_jobs = -1, **kwargs):
    '''
    Trains an AutoSklearnClassifier model with specific optimization and ensemble settings.
    
    Args:
        dataset_name (str): 
            the name of the dataset, which will be used in the model training log and fit().
        model_path (str): 
            the path to save the trained model.
        set_no (int):
            the set number of the training data, such as 1, 2, etc.
        channel (str): 
            the sensor channel to be trained, such as 'lr_left', 'ud_axial', etc.
        X_train (pd.DataFrame): 
            the training data features.
        X_test (pd.DataFrame): 
            the test data features.
        y_train (pd.DataFrame): 
            the training data labels.
        y_test (pd.DataFrame): 
            the test data labels.
        time_limit (int): 
            the total time limit for the model training in seconds. Default is 600 seconds.
        per_run_limit (int): 
            the time limit for each model training run in seconds. Default is 200 seconds.
        n_jobs (int): 
            the number of jobs to run in parallel. Default is -1, which means using all available cores.
        **kwargs:
            additional keyword arguments to be passed to the AutoSklearnClassifier.
    Returns:
        None
    Examples:
        >>> train_autosklearn_v1_model(dataset_name='DS_L_01,PP_HR_400', model_path='model_v1.joblib', set_no=1, channel='lr_left',
                                      X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test,
                                      time_limit=600, per_run_limit=200, n_jobs=-1, 
                                      ensemble_kwargs={'ensemble_size': 5}, ensemble_nbest=10, 
                                      metric=metrics.CLASSIFICATION_METRICS['f1_weighted'])
    '''
    import autosklearn.classification
    automlclassifierV1 = autosklearn.classification.AutoSklearnClassifier(
        time_left_for_this_task=time_limit,
        per_run_time_limit=per_run_limit,
        resampling_strategy='cv',
        resampling_strategy_arguments={'folds': 5},
        memory_limit=None,
        n_jobs=n_jobs,
        **kwargs
    )
    t1 = time.time()
    automlclassifierV1.fit(X_train, y_train, dataset_name=dataset_name)
    t2 = time.time()
    # print log
    model_training_log(automlclassifierV1, model_path.split('/')[-1], 
                       X_train, y_train, X_test, y_test, 
                       actual_training_time=t2-t1, dataset_name=dataset_name, train_test_split=set_no,channel=channel)
    save_model(automlclassifierV1, model_path)
    
def train_autosklearn_v2_model(dataset_name:str, model_path:str, set_no:int, channel:str, 
                               X_train, X_test, y_train, y_test, 
                               time_limit = 600, per_run_limit = 200, n_jobs = -1, **kwargs):
    '''
    Trains an AutoSklearn2Classifier model with specific optimization and ensemble settings.
    
    Note:

    Due to a [known bug](https://github.com/automl/auto-sklearn/issues/1654) in auto-sklearn v0.15.0 
    preventing direct optimization with f1_weighted or f1_macro metrics, the model's primary optimization
    goal (AutoML search process) is set to 'accuracy' by default by leaving `metric` argument as default.

    However, the ensemble building phase explicitly incorporates 'f1_weighted'
    via `ensemble_kwargs`. This ensures that while the overall model selection
    is accuracy-driven, the final ensemble's sub-model weighting prioritizes
    f1_weighted performance, providing a degree of f1-score orientation.
    Args:
        dataset_name (str): 
            the name of the dataset, which will be used in the model training log and fit().
        model_path (str): 
            the path to save the trained model.
        set_no (int):
            the set number of the training data, such as 1, 2, etc.
        channel (str): 
            the sensor channel to be trained, such as 'lr_left', 'ud_axial', etc.
        X_train (pd.DataFrame): 
            the training data features.
        X_test (pd.DataFrame): 
            the test data features.
        y_train (pd.DataFrame): 
            the training data labels.
        y_test (pd.DataFrame): 
            the test data labels.
        time_limit (int): 
            the total time limit for the model training in seconds. Default is 600 seconds.
        per_run_limit (int): 
            the time limit for each model training run in seconds. Default is 200 seconds.
        n_jobs (int): 
            the number of jobs to run in parallel. Default is -1, which means using all available cores.
        **kwargs:
            additional keyword arguments to be passed to the AutoSklearn2Classifier.
    Returns:
        None
    Examples:
        >>> train_autosklearn_v2_model(dataset_name='DS_L_01,PP_HR_400', model_path='model_v2.joblib', set_no=1, channel='lr_left',
                                      X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test,
                                      time_limit=600, per_run_limit=200, n_jobs=-1, 
                                      ensemble_kwargs={'ensemble_size': 5, 'metrics': metrics.CLASSIFICATION_METRICS['f1_weighted']},
                                      ensemble_nbest=10)
    '''
    from autosklearn.experimental.askl2 import AutoSklearn2Classifier

    automlclassifierV2 = AutoSklearn2Classifier(
        time_left_for_this_task=time_limit,
        per_run_time_limit=per_run_limit,
        memory_limit=None,
        n_jobs=n_jobs,
        **kwargs
    )
    t1 = time.time()
    automlclassifierV2.fit(X_train, y_train, dataset_name = dataset_name)
    t2 = time.time()
    # print log
    model_training_log(automlclassifierV2, model_path.split('/')[-1], 
                       X_train, y_train, X_test, y_test, 
                       actual_training_time=t2-t1, dataset_name=dataset_name, train_test_split=set_no, channel=channel)
    save_model(automlclassifierV2, model_path)
    
def save_model(trained_model, path:str):
    from joblib import dump
    dump(trained_model, path)
    print('model saved at:', path)

test_sample = {
    'set1' : ['000027', '000048', '000053', '003735', '004073', '000785'],
    'set2' : ['000030', '000050', '003735', '003861', '001833', '002577'],
    'set3' : ['000030', '000039', '000052', '004072', '004073', '004802']
}

def train_models(dataset_name:str, dir:str, format:Literal['excel', 'parquet'],
                 channel:str, set_no:int, col:int, stats:bool=False, model_save_path:str='', high_resolution:bool=False):
    '''
    train models with autosklearn v1 and v2

    Args:
        dataset_name (str): 
            the name of the dataset, which will be used in the model training log and fit().
        dir (str): 
            directory of the psd spectrum data, which is in parquet format.
        channel (str):
            the sensor channel to be trained, such as 'lr_left', 'ud_axial', etc.
        set_no (str):
            the set number of the training data, such as 'set1', 'set2', etc.
        col (int):
            the number of features to be preserved, which is the number of columns in the psd spectrum.
        stats (bool):
            whether to add additional features such as mean and std of specific features.
            If True, the feature number will be increased by 2.
        model_save_path (str):
            the path to save the trained model. If None, the model will be saved at local directory.
        high_resolution (bool):
            whether the psd spectrum is high resolution, which will affect the model file name.
    Examples:
        >>> train_models(dataset_name='DS_L_01,PP_HR_400', dir='../../test_data//psd_20%//psd_window_high_resolution_20%//', 
                    format='parquet',
                    channel=channel, set_no=set_no, col=400, stats=False, 
                    model_save_path='../../model//20duty_high_resolution//', high_resolution=True,
                    ensemble_kwargs = {'ensemble_size': 5}, ensemble_nbest=10, metric=autosklearn.metrics.f1_weighted
                    )
    '''
    df = load_data(format=format, dir=dir, keyword=channel)
    df = preprocess_features(df, col=col, stats=stats)

    if df.isna().any().any():
        raise ValueError('nan values exist in the teat data.')
    
    X_train, X_test, y_train, y_test = train_test_split(df, test_samples=test_sample['set%d'%set_no])
    print(X_test.columns)
    
    if high_resolution:
        model_save_path += channel + '_high_resolution_' + 'set' + str(set_no) + '_' + str(col)
    else:
        model_save_path += channel + '_set' + str(set_no) + '_' + str(col)
    
    train_autosklearn_v1_model(dataset_name, model_save_path + '_v1.joblib', set_no, channel,  X_train, X_test, y_train, y_test, 
                               ensemble_kwargs = {'ensemble_size': 5}, ensemble_nbest=10, 
                               metric=metrics.CLASSIFICATION_METRICS['f1_weighted'])
    # use default metric for autosklearn v2, because f1_weighted is not supported in autosklearn v2
    train_autosklearn_v2_model(dataset_name, model_save_path + '_v2.joblib', set_no, channel, X_train, X_test, y_train, y_test,
                               ensemble_kwargs = {'ensemble_size': 5, 'metrics': metrics.CLASSIFICATION_METRICS['f1_weighted']},
                               ensemble_nbest=10)

def sample_from_df(df:pd.DataFrame, frac:float):
    '''
    sample from the dataframe, return the sample number of the sampled data.
    Args:
        df (pd.DataFrame): 
            the dataframe containing the psd spectrum data. Contains a column 'sample_num' which is the sample number.
        frac (float): 
            the fraction of samples to be sampled from the dataframe.
    '''
    all_samples = df['sample_num'].value_counts()
    return all_samples.sample(frac=frac).index.to_list()

def evaluate_model(model: object, X_test: pd.DataFrame, y_test: pd.DataFrame, 
                   draw_plot:bool = True):
    ''' Evaluate the model's performance on the test set and print the results.
    Args:
        model (object): 
            the trained model to be evaluated. The model should have `decision_function` and `predict` methods.
            Note: the anomaly is predicted as 1, and normal is predicted as 0.
        X_test (pd.DataFrame): 
            the test data features.
        y_test (pd.DataFrame): 
            the test data labels.
        draw_plot (bool):
            whether to draw the evaluation plots, such as confusion matrix, ROC curve, and Precision-Recall curve.
    Returns:
        metrics (dict):
            a dictionary containing all the evaluation metrics.
        plots (dict):
            a dictionary containing the evaluation plots, such as confusion matrix, ROC curve, and Precision-Recall curve.
    '''
    from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, f1_score, confusion_matrix, ConfusionMatrixDisplay
    
    y_test_scores = model.decision_function(X_test)
    # default y_test_pred is the prediction based on the model's internal threshold
    # this is used for initial f1-score calculation
    y_test_pred = model.predict(X_test)
    
    # evaluate and print the results
    roc_auc = roc_auc_score(y_test, y_test_scores)
    precision, recall, thresholds = precision_recall_curve(y_test, y_test_scores)
    pr_auc = auc(recall, precision)
    
    # find the optimal threshold based on F1-score
    # Note: thresholds has one less element than precision/recall since it represents the threshold 
    # for the next precision/recall pair
    f_scores_at_thresholds = 2 * (precision * recall) / (precision + recall + 1e-10) # add small value to avoid division by zero
    
    # in case of all precision or recall is zero, f_scores_at_thresholds will be all zeros
    if np.all(f_scores_at_thresholds == 0):
        print("all F1-score is zero, no optimal threshold found.")
        optimal_f1_idx = 0 # fallback to the first threshold
    else:        
        optimal_f1_idx = np.argmax(f_scores_at_thresholds)
    
    optimal_threshold = thresholds[optimal_f1_idx]
    optimal_precision = precision[optimal_f1_idx]
    optimal_recall = recall[optimal_f1_idx]
    optimal_f1_score = f_scores_at_thresholds[optimal_f1_idx]
    y_test_pred_optimal_f1 = (y_test_scores >= optimal_threshold).astype(int)
    
    # calculate metrics for the model's internal threshold
    f1_score_internal = f1_score(y_test, y_test_pred)
    predicted_anomalies_internal = np.sum(y_test_pred)

    # calculate metrics for the optimal F1 threshold
    cm_optimal_f1 = confusion_matrix(y_test, y_test_pred_optimal_f1)
    predicted_anomalies_optimal_f1 = np.sum(y_test_pred_optimal_f1)
    
    # Prepare metrics dictionary
    metrics = {
        'ROC_AUC': roc_auc,
        'PR_AUC': pr_auc,
        'Optimal_F1_Score': optimal_f1_score,
        'Optimal_Threshold': optimal_threshold,
        'Precision_at_Optimal_F1': optimal_precision,
        'Recall_at_Optimal_F1': optimal_recall,
        'Predicted_Anomalies_Optimal_F1': int(predicted_anomalies_optimal_f1),
        # store confusion matrix as a dictionary (for JSON logging) in metrics, insure INT type
        'Confusion_Matrix_Optimal_F1': {
            'TN': int(cm_optimal_f1[0, 0]),   'FP': int(cm_optimal_f1[0, 1]),
            'FN': int(cm_optimal_f1[1, 0]),   'TP': int(cm_optimal_f1[1, 1]),
        },
        'Model_Default_F1_Score': f1_score_internal,
        'Model_Default_Predicted_Anomalies': int(predicted_anomalies_internal),
    }

    plots = {}
    if draw_plot:
        # draw confusion matrix for the model's optimal F1 threshold
        cm_display = ConfusionMatrixDisplay(confusion_matrix=cm_optimal_f1, 
                                            display_labels=np.unique(y_test)).plot(cmap='Blues')
        cm_display.ax_.set_title(f'Confusion Matrix for {model.__class__.__name__} at Optimal F1 Threshold ({optimal_threshold:.4f})')
        plots['confusion_matrix'] = cm_display.figure_
        plt.show()

        # draw ROC and Precision-Recall curves
        fig, (ax1, ax2) = plt.subplots(1, 2, layout='constrained', figsize=(12, 6))
        
        # ROC curve
        from sklearn.metrics import roc_curve, RocCurveDisplay
        fpr, tpr, roc_thresholds = roc_curve(y_test, y_test_scores)
        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name= model.__class__.__name__)
        roc_display.plot(ax=ax1)
        # mark the threshold with maximum F1 score
        idx  = np.argmin(np.abs(roc_thresholds - optimal_threshold))
        ax1.plot(fpr[idx], tpr[idx], 'o', markersize=5, color='red',
                 label=f'Optimal F1 Threshold point={optimal_threshold:.4f}')
        ax1.legend(loc='lower right')
        ax1.set_title('ROC Curve')

        # Precision-Recall curve
        from sklearn.metrics import PrecisionRecallDisplay
        pr_display = PrecisionRecallDisplay(precision=precision, recall=recall, average_precision=pr_auc,
                                            estimator_name=model.__class__.__name__)
        pr_display.plot(ax=ax2)
        # mark the threshold with maximum F1 score
        ax2.plot(optimal_recall, optimal_precision, 'o', markersize=5, color='red',
             label=f'Optimal F1={optimal_f1_score:.4f}\n(P={optimal_precision:.4f}, R={optimal_recall:.4f}, Thresh={optimal_threshold:.4f})')
        
        ax2.legend(loc='lower left')
        ax2.set_title('Precision-Recall Curve')
        plots['roc_pr_curve'] = fig
        plt.show()
    
    return metrics, plots

def log_anomaly_detection_run(model_name: str, model_instance: object, 
                              hyperparameters: dict,
                              input_example: pd.DataFrame, # Added for logging model
                              training_time_sec: float,
                              dataset_id:str,
                              metrics: dict,
                              plots: dict,
                              test_samples_num: list,
                              preprocessing_id: str = 'N/A', 
                              feature_transformer_id: str = 'None',
                              good_data_train_split: str = 'N/A',
                              channel: str = 'mic0',
                              notes: str = '',
                              hardware_environment: str = 'Win10/cDAQ9171/NI-9234',
                              ):
    ''' Log the anomaly detection run details and evaluate the model to MLflow.
    Args:
        model_name (str): 
            the name of the model, which will be used in the MLflow run name.
        model_instance (object): 
            the trained model instance to be logged.
        input_example (pd.DataFrame): 
            an example input for the model, used to infer the model signature.
            This should be a single row of the test data, which will be used to log the model.
            It is used to infer the model signature for logging purposes.
            This is important for logging the model in MLflow.
            The input_example should be a DataFrame with the same structure as the training data.
        training_time_sec (float): 
            the total training time of the model in seconds.
        metrics (dict): 
            a dictionary containing evaluation metrics of the model, such as ROC_AUC, PR_AUC, etc.
        plots (dict): 
            a dictionary containing plots to be logged, such as confusion matrix, ROC curve, etc.
            Note: plots should be in the format {'confusion_matrix': cm_display, 'roc_curve': roc_display, ...}
            where cm_display and roc_display are matplotlib figure objects.
            These plots will be saved as images in the MLflow run artifacts.
        hyperparameters (dict): 
            a dictionary containing hyperparameters of the model.
        dataset_id (str): 
            the dataset ID used for logging purposes.
        test_samples_num (list): 
            a list of sample numbers used for testing.
        preprocessing_id (str, optional): 
            the preprocessing ID used for logging purposes. Defaults to 'N/A'.
        feature_transformer_id (str, optional): 
            the feature transformer ID used for logging purposes. Defaults to 'None'.
        good_data_train_split (str, optional): 
            the train-test split method used for logging purposes (e.g., '80%'). Defaults to 'N/A'.
        channel (str, optional): 
            the sensor channel used for logging purposes. Defaults to 'mic0'.
        notes (str, optional): 
            additional notes for the run. Defaults to ''.
        hardware_environment (str, optional): 
            hardware environment information. Defaults to 'Win10/cDAQ9171/NI-9234'.
    '''
    with mlflow.start_run(run_name=model_name) as run:
        mlflow.log_param("model_name", model_name)
        mlflow.log_params(hyperparameters)
        mlflow.log_param("training_time_sec", training_time_sec)
        mlflow.log_param("dataset_id", dataset_id)
        mlflow.log_param("preprocessing_id", preprocessing_id)
        mlflow.log_param("feature_transformer_id", feature_transformer_id)
        mlflow.log_param("good_data_train_split", good_data_train_split)
        mlflow.log_param("channel", channel)
        mlflow.log_param("hardware_environment", hardware_environment)
        mlflow.log_param("test_samples_num", str(test_samples_num))
        mlflow.log_param("notes", notes)
        # since log_metrics only accepts float values, we need to convert the metrics dictionary
        # to a dictionary with float values
        cm = metrics.pop('Confusion_Matrix_Optimal_F1', {})  
        mlflow.log_params(cm)
        mlflow.log_metrics(metrics)
        
        # get a sample output for signature inference
        # if model_instance.decision_function or predict() returns a 1D array, take the first element
        sample_output = model_instance.decision_function(input_example) if hasattr(model_instance, 'decision_function') else model_instance.predict(input_example)
        sample_output = sample_output[0] if isinstance(sample_output, np.ndarray) and sample_output.ndim > 1 else sample_output
        # log the model instance
        signature = mlflow.models.infer_signature(input_example, sample_output) #output is a single float score
        mlflow.sklearn.log_model(model_instance, name="model", input_example=input_example,
                                 signature=signature, registered_model_name=model_name)
        
        # log plots as artifacts
        for plot_name, fig_obj in plots.items():
            mlflow.log_figure(fig_obj, f"plots/{plot_name}.png")
        
        print(f"Run {run.info.run_id} logged successfully with model: {model_name}")
        print(f"Run URL: {mlflow.get_tracking_uri()}")
    
def Novelty_detection():
    '''
    detect novelty in the psd spectrum
    '''
    from pyod.models.ocsvm import OCSVM
    from pyod.models.iforest import IForest
    from pyod.models.lof import LOF
    #from pyod.models.auto_encoder import AutoEncoder
    from pyod.utils.data import evaluate_print
    # 1. 訓練集只包含良品數據
    def parse_func(filename:str):
        '''
        parse the filename to get the sample number
        '''
        return filename.split('_')[3]
    df = load_data(format='parquet', dir='../../test_data//20250623_test_samples//psd_20%_window//', 
                   keyword='mic', parse_func=parse_func)
    
    print(df.head())
    abnormal_samples_num = ['b00053', 'b04802']
    good_samples_psd_data = df.loc[[x not in abnormal_samples_num for x in df['sample_num']]]
    
    # 2. 構建測試集 (包含未見過的良品和所有不良品)
    test_samples_num = sample_from_df(good_samples_psd_data, 0.3) # 30% of good samples for validation
    def label_method(sample_num: str):
        '''
        transfer sample number to label 0 and 1
        '''
        return 0 if sample_num not in abnormal_samples_num else 1
    
    df = preprocess_features(df, col=(50,170))
    X_train_good, X_test, y_train_good, y_test = train_test_split(df, test_samples_num+abnormal_samples_num, label_mothod=label_method)
    
    input_example = X_test.iloc[[0]]  # Use the first row of X_test as an example input for logging
    test_samples = test_samples_num + abnormal_samples_num
    contamination = 0.05
    dataset_id = 'DS_L_04'  # dataset ID for logging purposes
    preprocessing_id = 'PP_LR_512'  # preprocessing ID for logging purposes
    feature_transformer_id = 'None'  # feature transformer ID for logging purposes
    good_data_train_split = '70%'  # train-test split method for logging purposes
    # 3. compare different novelty detection methods
    # OCSVM
    print('\n--- Training OCSVM ---')
    start_time = time.time()
    model_ocsvm = OCSVM(nu=0.01, contamination=contamination)
    model_ocsvm.fit(X_train_good)
    training_time_ocsvm = time.time() - start_time
    metrics_ocsvm, plots_ocsvm = evaluate_model(model_ocsvm, X_test, y_test)
    log_anomaly_detection_run(
        model_name=model_ocsvm.__class__.__name__,
        model_instance=model_ocsvm,
        hyperparameters=model_ocsvm.get_params(),
        input_example=input_example,
        training_time_sec=training_time_ocsvm,
        dataset_id=dataset_id,
        metrics=metrics_ocsvm,
        plots=plots_ocsvm,
        test_samples_num=test_samples,
        preprocessing_id=preprocessing_id,
        feature_transformer_id=feature_transformer_id,
        good_data_train_split=good_data_train_split,
    )
    # Isolation Forest
    print('\n--- Training Isolation Forest ---')
    start_time = time.time()
    model_iforest = IForest(contamination=contamination, random_state=42)
    model_iforest.fit(X_train_good)
    training_time_iforest = time.time() - start_time
    metrics_iforest, plots_iforest = evaluate_model(model_iforest, X_test, y_test)
    log_anomaly_detection_run(
        model_name=model_iforest.__class__.__name__,
        model_instance=model_iforest,
        hyperparameters=model_iforest.get_params(),
        input_example=input_example,
        training_time_sec=training_time_iforest,
        dataset_id=dataset_id,
        metrics=metrics_iforest,
        plots=plots_iforest,
        test_samples_num=test_samples,
        preprocessing_id=preprocessing_id,
        feature_transformer_id=feature_transformer_id,
        good_data_train_split=good_data_train_split,
    )
    # Local Outlier Factor
    print('\n--- Training Local Outlier Factor ---')
    start_time = time.time()
    model_lof = LOF(contamination=contamination, n_neighbors=20, novelty=True)
    model_lof.fit(X_train_good)
    training_time_lof = time.time() - start_time
    metrics_lof, plots_lof = evaluate_model(model_lof, X_test, y_test)
    log_anomaly_detection_run(
        model_name=model_lof.__class__.__name__,
        model_instance=model_lof,
        hyperparameters=model_lof.get_params(),
        input_example=input_example,
        training_time_sec=training_time_lof,
        dataset_id=dataset_id,
        metrics=metrics_lof,
        plots=plots_lof,
        test_samples_num=test_samples,
        preprocessing_id=preprocessing_id,
        feature_transformer_id=feature_transformer_id,
        good_data_train_split=good_data_train_split,
    )
    '''
    # AutoEncoder
    model_ae = AutoEncoder( hidden_neurons=[64, 32, 16, 32, 64], 
                            hidden_activation='relu',
                            output_activation='sigmoid', # 如果數據經過正規化到 [0,1]，則用 sigmoid
                            loss='mse',
                            optimizer='adam',
                            epochs=50,
                            batch_size=32,
                            contamination=contamination,
                            random_state=42,
                            verbose=0)
    model_ae.fit(X_train_good)
    evaluate_model(model_ae, X_test, y_test)
    
    '''

def view_model_pipeline(model_file_name: str):
    '''
    view the model pipeline
    Args:
        model_file_name (str): 
            the file name of the trained model, which is a joblib file.
    '''
    import autosklearn.classification as asc

    # 假設你已經訓練好了 automl 模型
    # automl = asc.AutoSklearnClassifier(...)
    # automl.fit(...)

    # 1. 獲取最佳 pipeline 的詳細信息
    # 這會返回一個字典，其中包含每個 pipeline 的 ID 和其對應的物件
    best_model_id = automl.leaderboard().index[0] # 通常 leaderboard 的第一行就是最佳模型
    best_pipeline = automl.show_models()[best_model_id]

    # 2. 解析 pipeline 結構
    # auto-sklearn 的 pipeline 是一個特殊的類型，但你可以訪問其內部結構。
    # 通常，特徵轉換器會被包裝在一個 'feature_preprocessor' 或 'data_preprocessor' 步驟中。
    # 需要根據 best_pipeline 的具體結構來判斷。

    # 這裡假設 auto-sklearn 的 pipeline 結構類似 sklearn.pipeline.Pipeline
    # 最佳 pipeline 的內部結構可能會比較複雜，它可能是一個 AutoSklearnManagedPipeline
    # 你需要檢查這個對象的屬性。

    # 一種常見的訪問方式是通過 `steps` 屬性（如果它是一個 sklearn Pipeline-like object）
    # 或者通過 automl 內部維護的 pipeline 對象
    # 檢查 https://automl.github.io/auto-sklearn/master/api.html#autosklearn.classification.AutoSklearnClassifier.show_models

    # 示例：假定 best_pipeline_obj 是你獲取到的實際 Pipeline 物件
    # (注意：直接從 show_models() 得到的字典可能需要進一步處理才能得到 Pipeline 物件)
    # 假設你已經提取到類似 sklearn.pipeline.Pipeline 的物件
    # 比如通過 automl._automl._backend.load_pipeline(run_key)

    # 假設 pipeline_obj 是已經載入的 scikit-learn Pipeline 物件
    # for step_name, step_transformer in pipeline_obj.steps:
    #     if "feature_preprocessor" in step_name or "data_preprocessor" in step_name:
    #         feature_transformer = step_transformer
    #         print(f"找到特徵轉換器: {step_name} - {feature_transformer}")
    #         break

    # 更可靠的方法是從 `automl.get_models_with_weights()` 中獲取實際的 scikit-learn 模型對象
    for weight, model in automl.get_models_with_weights():
        # 這裡 model 是一個 sklearn.pipeline.Pipeline 的實例
        # 你可能需要找到權重最高的那個模型，或者檢查每一個
        if weight > 0: # 獲取權重不為零的模型
            print(f"處理權重為 {weight} 的模型...")
            for name, step in model.steps:
                # 判斷是否為特徵轉換步驟
                # 常見的特徵轉換步驟名稱可能包含 'data_preprocessor', 'feature_preprocessor', 'reduction' 等
                # 你可以根據 auto-sklearn 可能使用的轉換器類型來判斷，例如 PCA, RBF kernelizer 等
                if hasattr(step, 'transform') and not hasattr(step, 'predict'):
                    print(f"  找到潛在的特徵轉換步驟: {name}, Transformer: {step.__class__.__name__}")
                    # 如果這是你需要的特徵轉換器，你可以保存它
                    # feature_transformer = step
                    # break # 如果你只想要第一個

            # 你也可以直接訪問 pipeline 中的特定步驟
            # 例如，如果 auto-sklearn 使用了 PCA 作為 feature_preprocessor
            # if 'feature_preprocessor' in model.named_steps:
            #     pca_transformer = model.named_steps['feature_preprocessor']
            #     print(f"提取的 PCA 轉換器: {pca_transformer}")
            #     # 你現在可以使用 pca_transformer.transform(your_new_data) 來進行特徵轉換

if __name__ == '__main__':
    # Set MLflow tracking URI
    mlflow.set_tracking_uri('file:./mlruns')
    mlflow.set_experiment('Novelty Detection Experiment on Sound Data PSD Spectrum use feature order 50-170')
    Novelty_detection()