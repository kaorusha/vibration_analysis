import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import signal_processing
from typing import Literal
import time
from joblib import load
from sklearn.metrics import classification_report
import autosklearn.metrics

def shapley_value(x_train:np.ndarray, x_test:np.ndarray, y_train:np.ndarray, y_test:np.ndarray, X:pd.DataFrame):
    from sklearn.metrics import accuracy_score, mean_squared_error
    import shap
    from xgboost.sklearn import XGBRegressor

    shap.initjs()
    xgb_model = XGBRegressor(n_estimators=1000, max_depth=10, learning_rate=0.001, random_state=0)
    xgb_model.fit(x_train, y_train)
    y_predict = xgb_model.predict(x_test)
    print(y_predict)
    mean_squared_error(y_test, y_predict)**(0.5)
    explainer = shap.TreeExplainer(xgb_model)
    shap_values = explainer.shap_values(x_train)
    shap.summary_plot(shap_values, features=x_train, feature_names=X.columns)
    acc = accuracy_score(y_test, y_predict)
    print(acc)

def fisher_score_show(x_train:np.ndarray, x_test:np.ndarray, y_train:np.ndarray, y_test:np.ndarray):
    from skfeature.function.similarity_based import fisher_score
    from sklearn import svm
    from sklearn.metrics import accuracy_score

    score = fisher_score.fisher_score(x_train, y_train)
    idx = fisher_score.feature_ranking(score)
    num_fea = 5
    
    plt.figure(layout="constrained")
    plt.bar(range(len(score)), score)
    plt.xlabel('Feature Index')
    plt.ylabel('Fisher Score')
    plt.title('Fisher Score for Each Feature')
    for i in range(num_fea):
        plt.annotate(idx[i], 
                     xy=(idx[i], score[idx[i]]), rotation=0, xycoords='data',
                     xytext=(0,0), textcoords='offset pixels')
    plt.show()
    
    selected_feature_train = x_train[:, idx[0:num_fea]]
    selected_feature_test = x_test[:, idx[0:num_fea]]
    clf = svm.LinearSVC(dual=False)
    clf.fit(selected_feature_train, y_train)
    y_predict = clf.predict(selected_feature_test)
    acc = accuracy_score(y_test, y_predict)
    print(acc)

def compare_target_predict(df:pd.DataFrame, target:str, predict:str):
    '''
    show wrong prediction spectrum

    Parameters
    ----------
    df : input dataframe
    target : str
        name pf target column
    predict : str
        name of predict column
    
    Examples
    --------
    >>> compare_target_predict(df, 'Type', 'Predict')
    '''
    df_wrong = df.loc[df[target] != df[predict], :]
    fig, axs = plt.subplots(1,1, layout='constrained')
    for i in df_wrong.index:
        axs.plot(df_wrong.loc[i][:151], label= i + ': target %i predict %i'%(df_wrong.loc[i, 'Type'], df_wrong.loc[i, 'Predict']))
    axs.set_xticks(np.arange(0,151, step=10))
    axs.set_yscale('log')
    axs.legend()
    plt.show()

# (Moved above to fix default argument error)
def label_transfer(sample_num: str):
    '''
    transfer sample number to label 0 and 1
    '''
    return 0 if signal_processing.class_label(sample_num) == 0 else 1

def predict(psd_file:str, joblib:str, keyword:str, col:int, stats:bool, label_method = label_transfer):
    '''
    load psd spectrum, and load model, output the predict result

    Args:
        psd_file(str):
            psd spectrum generated by `acc_processing_excel()`. The samples are saved in separated sheets, 
            the column label in each sheets is the sensor channel, and the index is the order number.
            The psd result is averaged by windows.
        joblib(str): 
            file name of the trained classification model
        keyword(str):
            selected sensor channel
        col(int):
            controls the feature number to match the model feature number
        stats(bool):
            whether to add additional features such as mean and std of specific features.
            If True, the feature number will be increased by 2.
    Examples:
        >>> predict(psd_file='../../test_data//20250410_test_samples//psd_20%//psd_high_resolution.xlsx',
                    joblib='../../model//20duty_high_resolution//lr_left_high_resolution_set1_5121_v1.joblib',
                    keyword='lr_left', col=5121, stats=False)
    '''
    # 匯入模型
    clf = load(joblib)
    
    df = load_data(window=False, format='excel', dir=psd_file, keyword=keyword)

    res = df['sample_num'].to_frame()
    res['label'] = df['sample_num'].apply(label_method)

    # 模型預測測試
    t1 = time.time()
    df = preprocess_features(df, col=col, stats=stats)
    res['predict'] = clf.predict(df.drop(columns=['sample_num']))
    t2 = time.time()
    total_inference_time_ms = (t2 - t1) * 1000
    
    print(joblib.split('/')[-1][:-7])
    print(res)
    print(classification_report(res['label'], res['predict']))
    print(f'Total inference time: {total_inference_time_ms:.2f} ms')
        
def predict_single(psd_file:str, joblib:str, keyword:str, col:int, stats:bool, label_method = label_transfer):
    # 匯入模型
    clf = load(joblib)
    
    df = load_data(window=False, format='excel', dir=psd_file, keyword=keyword)

    total_samples = df['sample_num'].shape[0]
    total_inference_time_ms = 0
    res = df['sample_num'].to_frame()
    res['label'] = df['sample_num'].apply(label_method)

    for i in range(total_samples):
        single_sample = df.iloc[i, :].to_frame().transpose()
        # 模型預測測試
        t1 = time.time()
        single_sample = preprocess_features(single_sample, col=col, stats=stats)
        target = clf.predict(single_sample.drop(columns=['sample_num']))
        t2 = time.time()
        total_inference_time_ms += (t2 - t1) * 1000
        res.loc[i, 'predict'] = target
    
    print(joblib.split('/')[-1][:-7])
    print(res)
    print(classification_report(res['label'], res['predict']))
    print(f'Total inference time: {total_inference_time_ms:.2f} ms')
    print(f'Average inference time per sample: {total_inference_time_ms / total_samples:.2f} ms')
    
def load_data(format:Literal['excel', 'parquet'], dir:str, keyword:str, window:bool = True):
    '''
    load data of chosen format and filtered with keyword.
    Args:
        format (Literal['excel', 'parquet']):
            the format of the data, either 'excel' or 'parquet', which contains the psd spectrum.
        dir (str): the directory of the parquet file or the filename of the excel file.
        keyword (str): the channel of sensor, such as 'lr_left', 'lr_right', etc.
        window (bool): whether the data is windowed, default is True. If True, the data is read from un-averaged psd spectrum.
        
    Returns:
        pd.DataFrame: a dataframe containing the psd spectrum data. The columns are the order number of the psd spectrum,
        the sensor channel, and the sample number. The index is shuffled.
    Examples:
        >>> df = load_data(format='parquet', dir='../../test_data//psd_20%//psd_window_high_resolution_20%//', keyword='lr_left')
        >>> df.head()
    Raises:
        ValueError: if the dataframe contains NaN values.
    '''
    if format == 'excel':
        df = pd.DataFrame()
        if not window:
            df = signal_processing.read_sheets(filename=dir, usecols=[0,1,2,3], combine=True)
            df = df.transpose()
            df['name'] = df.index
            df = df.reset_index(drop=True)
        
        else:
            df = signal_processing.read_sheets(filename=dir, combine=True, axis=0)
        
        # add columns to describe the sensor channel and the sample_num
        df['channel'] = [name[7:] for name in df['name']]
        df['sample_num'] = [name[:6] for name in df['name']]
        # select a particular channel and shuffle
        df = df.loc[df['channel'] == keyword].drop(columns='channel').sample(frac = 1).reset_index(drop=True)
    
    else:
        # select a particular channel and shuffle
        df = signal_processing.read_parquet_keyword(keyword, dir, 
                                                parse_func=signal_processing.parse_digital).sample(frac=1).reset_index(drop=True)
    if df.isna().any().any():
        raise ValueError('nan values exist in the teat data.')
    return df
    
    
def preprocess_features(df:pd.DataFrame, col:int, stats:bool=False):
    '''
    preprocessing

    Parameters
    ----------
    df : dataframe contains 'sample_num' 
    col : preserved column number
    stats : bool
        whether to add additional features such as mean and std of specific features
    '''
    # add additional features such as mean and std of specific features
    if stats:
        df = signal_processing.calculate_spectral_stats(30, 80, df)
        # drop unused columns to reduce feature number
        df = pd.concat([df.iloc[:, :col], df.iloc[:, -3:]], axis=1)
    else:
        df = pd.concat([df.iloc[:, :col], df['sample_num']], axis=1)
    # transfer the column label into string before training
    df = signal_processing.cast_column_to_str(df, 2)
    if df.isna().any().any():
        raise ValueError('nan values exist in the teat data.')
        
    return df

def train_test_split(df:pd.DataFrame, test_samples: list):
    '''
    separate train and test set

    Parameters
    ----------
    df : dataframe contains 'sample_num'
    '''
    X_train = df.loc[[x not in test_samples for x in df['sample_num']]]
    X_test = df.loc[[x in test_samples for x in df['sample_num']]]

    y_train = np.array([label_transfer(sample_num) for sample_num in X_train['sample_num']])
    y_test = np.array([label_transfer(sample_num) for sample_num in X_test['sample_num']])

    # encode categorical columns
    y_train = pd.DataFrame(y_train, dtype="category")
    y_test = pd.DataFrame(y_test, dtype="category")

    # drop target
    X_train = X_train.drop(columns='sample_num').reset_index(drop=True)
    X_test = X_test.drop(columns='sample_num').reset_index(drop=True)
    print('train shape:', X_train.shape)
    print('test shape:', X_test.shape)
    return X_train, X_test, y_train, y_test

def model_info(model_file_name: str):
    '''
    load a trained model and print model info
    '''
    model = load(model_file_name)
    
    df = model.leaderboard(detailed = True, ensemble_only=True)
    print(df)
    # detail of the model
    best_model_info = model.show_models()
    print(best_model_info)
    model.sprint_statistics()
    
def model_training_log(model, model_file_name: str, X_train:pd.DataFrame, y_train:pd.DataFrame, X_test:pd.DataFrame, y_test:pd.DataFrame,
                       actual_training_time:float = 0.0, channel:str = 'unknown', ):
    '''
    print model training log for convenience pasting on table
    reference: training data summery table
    '''
    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())
    train_report_dict = classification_report(y_train, model.predict(X_train), output_dict=True)
    test_report_dict = classification_report(y_test, model.predict(X_test), output_dict=True)


    print(
        f"{timestamp} | {model_file_name} | {model.__class__.__name__} | {model} | {actual_training_time:.2f} | | | |"
        f"{channel}| | {train_report_dict['accuracy']:.4f} |  | {test_report_dict['accuracy']:.4f} | "
        f"{test_report_dict['weighted avg']['precision']:.4f} | {test_report_dict['weighted avg']['recall']:.4f} | "
        f"{test_report_dict['weighted avg']['f1-score']:.4f} "
    )
    

def train_autosklearn_v1_model(model_path:str, channel:str, X_train, X_test, y_train, y_test, time_limit = 600, per_run_limit = 200, n_jobs = -1, **kwargs):
    import autosklearn.classification
    automlclassifierV1 = autosklearn.classification.AutoSklearnClassifier(
        time_left_for_this_task=time_limit,
        per_run_time_limit=per_run_limit,
        resampling_strategy='cv',
        resampling_strategy_arguments={'folds': 5},
        memory_limit=None,
        n_jobs=n_jobs,
        **kwargs
    )
    t1 = time.time()
    automlclassifierV1.fit(X_train, y_train)
    t2 = time.time()
    # print log
    model_training_log(automlclassifierV1, model_path.split('/')[-1], X_train, y_train, X_test, y_test, actual_training_time=t2-t1, channel=channel)
    save_model(automlclassifierV1, model_path)
    
def train_autosklearn_v2_model(model_path:str, channel:str, X_train, X_test, y_train, y_test, time_limit = 600, per_run_limit = 200, n_jobs = -1, **kwargs):
    from autosklearn.experimental.askl2 import AutoSklearn2Classifier

    automlclassifierV2 = AutoSklearn2Classifier(
        time_left_for_this_task=time_limit,
        per_run_time_limit=per_run_limit,
        memory_limit=None,
        n_jobs=n_jobs
    )
    t1 = time.time()
    automlclassifierV2.fit(X_train, y_train)
    t2 = time.time()
    # print log
    model_training_log(automlclassifierV2, model_path.split('/')[-1], X_train, y_train, X_test, y_test, actual_training_time=t2-t1, channel=channel)
    save_model(automlclassifierV2, model_path)
    
def save_model(trained_model, path:str):
    from joblib import dump
    dump(trained_model, path)
    print('model saved at:', path)

test_sample = {
    'set1' : ['000027', '000048', '000053', '003735', '004073', '000785'],
    'set2' : ['000030', '000050', '003735', '003861', '001833', '002577'],
    'set3' : ['000030', '000039', '000052', '004072', '004073', '004802']
}

def train_models(dir:str, channel:str, set_no:str, col:int, stats:bool=False, model_save_path:str=None, high_resolution:bool=False, **kwargs):
    '''
    train models with autosklearn v1 and v2

    Args:
        dir (str): 
            directory of the psd spectrum data, which is in parquet format.
        channel (str):
            the sensor channel to be trained, such as 'lr_left', 'ud_axial', etc.
        set_no (str):
            the set number of the training data, such as 'set1', 'set2', etc.
        col (int):
            the number of features to be preserved, which is the number of columns in the psd spectrum.
        stats (bool):
            whether to add additional features such as mean and std of specific features.
            If True, the feature number will be increased by 2.
        model_save_path (str):
            the path to save the trained model. If None, the model will be saved at local directory.
        high_resolution (bool):
            whether the psd spectrum is high resolution, which will affect the model file name.
    Examples:
        >>> train_models(dir='../../test_data//psd_20%//psd_window_high_resolution_20%//', 
                    channel=channel, set_no=set_no, col=400, stats=False, 
                    model_save_path='../../model//20duty_high_resolution//', high_resolution=True,
                    ensemble_kwargs = {'ensemble_size': 5}, ensemble_nbest=10, metric=autosklearn.metrics.f1_weighted
                    )
    '''
    df = load_data(format='parquet', dir=dir, keyword=channel)
    df = preprocess_features(df, col=col, stats=stats)

    if df.isna().any().any():
        raise ValueError('nan values exist in the teat data.')
    
    X_train, X_test, y_train, y_test = train_test_split(df, test_samples=test_sample[set_no])
    print(X_test.columns)
    
    if high_resolution:
        model_save_path += channel + '_high_resolution_' + set_no + '_' + str(col)
    else:
        model_save_path += channel + set_no + '_' + str(col)
    
    train_autosklearn_v1_model(model_save_path + '_v1.joblib', channel,  X_train, X_test, y_train, y_test, **kwargs)
    train_autosklearn_v2_model(model_save_path + '_v2.joblib', channel, X_train, X_test, y_train, y_test, **kwargs)
    
if __name__ == '__main__':
    keyword = 'lr_left'
    set_no = 'set1'
    col = 400
    dir_model = '../../model//20duty_high_resolution_5ensemble//'
    
    # train models with autosklearn v1 and v2
    train_models(dir='../../test_data//psd_20%//psd_window_high_resolution_20%//',
                  channel=keyword, set_no=set_no, col=col, stats=False, 
                  model_save_path=dir_model, high_resolution=True, 
                  ensemble_kwargs = {'ensemble_size': 5}, ensemble_nbest=10, metric=autosklearn.metrics.f1_weighted)
    
    def label_test(sample_num: str):
        if sample_num in ['a00053', 'a03720', 'a04802', 'a01833']:
            return 1
        else:
            return 0
    
    versions = ['v1', 'v2']
    # predict the test samples with the trained models
    for version in versions:
        joblib = dir_model + '%s_high_resolution_%s_%d_%s.joblib'%(keyword, set_no, col, version)
        
        predict(psd_file='../../test_data//20250410_test_samples//psd_20%//psd_high_resolution.xlsx',
                joblib=joblib, keyword=keyword, col=col, stats=False, label_method=label_test)